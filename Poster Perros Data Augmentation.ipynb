{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se debe tener las últimas versiones de Keras y TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = True\n",
    "top_24 = True\n",
    "dog_breed_train_path = 'Dog Breed/train/'\n",
    "dog_breed_val_path = 'Dog Breed/val/'\n",
    "dog_breed_labels_path = 'Dog Breed/labels.csv'\n",
    "if top_24:\n",
    "    dog_breed_train_path = 'Dog Breed/train_top_24/'\n",
    "    dog_breed_val_path = 'Dog Breed/val_top_24/'\n",
    "\n",
    "# read addresses and labels\n",
    "labels = pd.read_csv(dog_breed_labels_path)\n",
    "\n",
    "# to shuffle data\n",
    "if shuffle_data:\n",
    "    labels = labels.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ojo: el siguiente bloque de código sólo se ejecuta una vez; sirve para mover los archivos de imágenes de cada clase a sus respectivas carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1000/8177\n",
      "Processing: original_index=5918, id=955fdc8f29b6b78c8888d0dfdd1c6a8d, label=mexican_hairless\n",
      "Train data: 2000/8177\n",
      "Processing: original_index=6993, id=af465eb9b2371bb3639518f0daa8529e, label=border_terrier\n",
      "Train data: 3000/8177\n",
      "Processing: original_index=6007, id=978a9eee19b794afb59da56af20e8714, label=blenheim_spaniel\n",
      "Train data: 4000/8177\n",
      "Processing: original_index=8480, id=d4dbe4468560bb227aac6627237ab9ec, label=bedlington_terrier\n",
      "Train data: 5000/8177\n",
      "Processing: original_index=8665, id=d96c7a1efc7455d533bb2beb4da27896, label=tibetan_mastiff\n",
      "Train data: 6000/8177\n",
      "Processing: original_index=5755, id=916acab38c9b1ef122851c0815c6826a, label=tibetan_mastiff\n",
      "Train data: 7000/8177\n",
      "Processing: original_index=2440, id=3cc8881795884439670f3b6722ad1482, label=african_hunting_dog\n",
      "Train data: 8000/8177\n",
      "Processing: original_index=2808, id=450652d3c78bcead07a39fd386dca46f, label=boxer\n",
      "Train data: 8177/8177\n",
      "Processing: original_index=9054, id=e381b4aaa19c31ef6765ab0d0af205c9, label=tibetan_terrier\n",
      "Validation data: 1000/2045\n",
      "Processing: original_index=2817, id=454aef984f2a2e3416aae8277b6b2422, label=beagle\n",
      "Validation data: 2000/2045\n",
      "Processing: original_index=2750, id=439e586066bf8ce8488b3927f60f0900, label=labrador_retriever\n",
      "Validation data: 2045/2045\n",
      "Processing: original_index=3528, id=57e15a54b72f8a5ea9abff0938dcfb95, label=scotch_terrier\n"
     ]
    }
   ],
   "source": [
    "# load dataset and labels into variables\n",
    "\n",
    "# use 20% of the train set to create a validation set and another 20% for a test set\n",
    "train_labels = labels.iloc[:int(0.8*len(labels))]\n",
    "val_labels = labels.iloc[int(0.8*len(labels)):]\n",
    "\n",
    "# a numpy array to save the mean of the images\n",
    "\n",
    "# loop over train addresses\n",
    "for i, (index, img_id, label) in enumerate(train_labels.itertuples()):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if (i+1) % 1000 == 0 or i+1 == len(train_labels):\n",
    "        print ('Train data: {0}/{1}'.format(i+1, len(train_labels)))\n",
    "        print('Processing: original_index={0}, id={1}, label={2}'.format(index, img_id, label))\n",
    "\n",
    "    # move the image to a subdirectory named after its label\n",
    "    if not os.path.isdir(os.path.join(dog_breed_train_path, label)):\n",
    "        os.mkdir(os.path.join(dog_breed_train_path, label))\n",
    "    os.rename(dog_breed_train_path+img_id+'.jpg', dog_breed_train_path+label+'/'+img_id+'.jpg')\n",
    "\n",
    "for i, (index, img_id, label) in enumerate(val_labels.itertuples()):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if (i+1) % 1000 == 0 or i+1 == len(val_labels):\n",
    "        print ('Validation data: {0}/{1}'.format(i+1, len(val_labels)))\n",
    "        print('Processing: original_index={0}, id={1}, label={2}'.format(index, img_id, label))\n",
    "\n",
    "    # move the image to a subdirectory named after its label\n",
    "    if not os.path.isdir(os.path.join(dog_breed_val_path, label)):\n",
    "        os.mkdir(os.path.join(dog_breed_val_path, label))\n",
    "    os.rename(dog_breed_train_path+img_id+'.jpg', dog_breed_val_path+label+'/'+img_id+'.jpg')\n",
    "\n",
    "# aqui va el centrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['scottish_deerhound', 126], dtype=object),\n",
       " array(['maltese_dog', 117], dtype=object),\n",
       " array(['afghan_hound', 116], dtype=object),\n",
       " array(['entlebucher', 115], dtype=object),\n",
       " array(['bernese_mountain_dog', 114], dtype=object),\n",
       " array(['shih-tzu', 112], dtype=object),\n",
       " array(['great_pyrenees', 111], dtype=object),\n",
       " array(['pomeranian', 111], dtype=object),\n",
       " array(['basenji', 110], dtype=object),\n",
       " array(['samoyed', 109], dtype=object),\n",
       " array(['airedale', 107], dtype=object),\n",
       " array(['tibetan_terrier', 107], dtype=object),\n",
       " array(['cairn', 106], dtype=object),\n",
       " array(['leonberg', 106], dtype=object),\n",
       " array(['beagle', 105], dtype=object),\n",
       " array(['japanese_spaniel', 105], dtype=object),\n",
       " array(['australian_terrier', 102], dtype=object),\n",
       " array(['blenheim_spaniel', 102], dtype=object),\n",
       " array(['miniature_pinscher', 102], dtype=object),\n",
       " array(['irish_wolfhound', 101], dtype=object),\n",
       " array(['lakeland_terrier', 99], dtype=object),\n",
       " array(['saluki', 99], dtype=object),\n",
       " array(['papillon', 96], dtype=object),\n",
       " array(['norwegian_elkhound', 95], dtype=object)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as sts\n",
    "\n",
    "freqs = sts.itemfreq(labels['breed'])\n",
    "\n",
    "freqs = sorted(freqs, key=lambda x: -x[1])\n",
    "\n",
    "(freqs[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2573"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for d,f in freqs[:24]:\n",
    "    s += f\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hay que jugar un poco con la arquitectura de la CNN. Mi PC no aguanta un modelo que tenga más de ~12 millones de parámetros, así que las opciones son achicar las imágenes de entrada (pic_dimension) y/o disminuir la cantidad de neuronas del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "n_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              6423552   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                24600     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 6,708,312\n",
      "Trainable params: 6,708,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic_dimension = 112\n",
    "\n",
    "modelo_1 = Sequential()\n",
    "modelo_1.add(Conv2D(64, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Flatten())\n",
    "modelo_1.add(Dense(1024))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Dense(n_classes))\n",
    "modelo_1.add(Activation('softmax'))\n",
    "modelo_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_1.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de datos del dataset es muy poco (10222 imagenes) respecto a la dimensionalidad del mismo. Por lo tanto, es útil aumentar artificialmente el dataset aplicando distorsiones aleatorias a cada imagen, de manera que el modelo en entrenamiento nunca vea la misma imagen más de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "80/80 [==============================] - 30s 370ms/step - loss: 3.2978 - acc: 0.0441 - val_loss: 3.1767 - val_acc: 0.0354\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 3.1755 - acc: 0.0594 - val_loss: 3.1225 - val_acc: 0.0979\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 3.1247 - acc: 0.0758 - val_loss: 3.0695 - val_acc: 0.1000\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 3.0637 - acc: 0.1026 - val_loss: 2.9336 - val_acc: 0.1771\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 14s 179ms/step - loss: 2.9613 - acc: 0.1141 - val_loss: 2.7965 - val_acc: 0.2021\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 13s 164ms/step - loss: 2.8872 - acc: 0.1409 - val_loss: 2.7888 - val_acc: 0.1625\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 2.8572 - acc: 0.1524 - val_loss: 2.7159 - val_acc: 0.2042\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 2.7788 - acc: 0.1641 - val_loss: 2.7344 - val_acc: 0.1854\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 2.7002 - acc: 0.1815 - val_loss: 2.6709 - val_acc: 0.2083\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 14s 181ms/step - loss: 2.6336 - acc: 0.1989 - val_loss: 2.4767 - val_acc: 0.2938\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 2.5886 - acc: 0.2147 - val_loss: 2.4382 - val_acc: 0.2667\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 2.5063 - acc: 0.2348 - val_loss: 2.3890 - val_acc: 0.2521\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 2.4881 - acc: 0.2432 - val_loss: 2.3802 - val_acc: 0.2771\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 14s 172ms/step - loss: 2.4281 - acc: 0.2653 - val_loss: 2.3768 - val_acc: 0.2812\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 15s 184ms/step - loss: 2.3861 - acc: 0.2666 - val_loss: 2.3093 - val_acc: 0.2833\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 2.3337 - acc: 0.2756 - val_loss: 2.3011 - val_acc: 0.2875\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 2.2597 - acc: 0.2945 - val_loss: 2.2688 - val_acc: 0.3063\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 2.2598 - acc: 0.3059 - val_loss: 2.6592 - val_acc: 0.2292\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 14s 177ms/step - loss: 2.1987 - acc: 0.3262 - val_loss: 2.3412 - val_acc: 0.2896\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 2.1247 - acc: 0.3467 - val_loss: 2.3289 - val_acc: 0.3083\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 2.1149 - acc: 0.3415 - val_loss: 2.2295 - val_acc: 0.3229\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 2.0699 - acc: 0.3585 - val_loss: 2.3122 - val_acc: 0.3208\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 2.0386 - acc: 0.3680 - val_loss: 2.2979 - val_acc: 0.3229\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 14s 180ms/step - loss: 1.9724 - acc: 0.3929 - val_loss: 2.2677 - val_acc: 0.3229\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 1.9359 - acc: 0.3979 - val_loss: 2.6367 - val_acc: 0.2646\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.8864 - acc: 0.4203 - val_loss: 2.3111 - val_acc: 0.3583\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 1.8821 - acc: 0.4082 - val_loss: 2.2059 - val_acc: 0.3708\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 1.8122 - acc: 0.4260 - val_loss: 2.2962 - val_acc: 0.3417\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 15s 183ms/step - loss: 1.8195 - acc: 0.4417 - val_loss: 2.4087 - val_acc: 0.3500\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 1.7539 - acc: 0.4530 - val_loss: 2.1787 - val_acc: 0.3292\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 1.7269 - acc: 0.4608 - val_loss: 2.6476 - val_acc: 0.3333\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.7107 - acc: 0.4629 - val_loss: 2.1211 - val_acc: 0.3708\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 1.6530 - acc: 0.4876 - val_loss: 2.3339 - val_acc: 0.3271\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 1.6395 - acc: 0.4819 - val_loss: 2.2531 - val_acc: 0.3542\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.6049 - acc: 0.4948 - val_loss: 2.2248 - val_acc: 0.3896\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 1.5173 - acc: 0.5219 - val_loss: 2.4066 - val_acc: 0.3438\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.5682 - acc: 0.5075 - val_loss: 2.2675 - val_acc: 0.3729\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 14s 177ms/step - loss: 1.4865 - acc: 0.5329 - val_loss: 2.1907 - val_acc: 0.4042\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 1.5277 - acc: 0.5190 - val_loss: 2.2980 - val_acc: 0.3854\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.4311 - acc: 0.5488 - val_loss: 2.2727 - val_acc: 0.3833\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 13s 166ms/step - loss: 1.4706 - acc: 0.5422 - val_loss: 2.1860 - val_acc: 0.3917\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 13s 169ms/step - loss: 1.4022 - acc: 0.5636 - val_loss: 2.2839 - val_acc: 0.4000\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 14s 181ms/step - loss: 1.3783 - acc: 0.5703 - val_loss: 2.5062 - val_acc: 0.3417\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 13s 167ms/step - loss: 1.3814 - acc: 0.5589 - val_loss: 2.3222 - val_acc: 0.3667\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 1.3215 - acc: 0.5859 - val_loss: 2.5519 - val_acc: 0.3583\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 1.3380 - acc: 0.5727 - val_loss: 2.2239 - val_acc: 0.3917\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 14s 173ms/step - loss: 1.2975 - acc: 0.5895 - val_loss: 2.2596 - val_acc: 0.3937\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 15s 185ms/step - loss: 1.3038 - acc: 0.5905 - val_loss: 2.4174 - val_acc: 0.4188\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 1.2269 - acc: 0.6113 - val_loss: 2.3243 - val_acc: 0.3917\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 1.2765 - acc: 0.6090 - val_loss: 2.4703 - val_acc: 0.3708\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_1.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_1.save_weights('modelo_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              10617856  \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                24600     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 10,902,616\n",
      "Trainable params: 10,902,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic_dimension = 150\n",
    "\n",
    "modelo_2 = Sequential()\n",
    "modelo_2.add(Conv2D(64, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_2.add(Activation('relu'))\n",
    "modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_2.add(Activation('relu'))\n",
    "modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_2.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_2.add(Activation('relu'))\n",
    "modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_2.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_2.add(Activation('relu'))\n",
    "modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_2.add(Flatten())\n",
    "modelo_2.add(Dense(1024))\n",
    "modelo_2.add(Activation('relu'))\n",
    "modelo_2.add(Dropout(0.3))\n",
    "modelo_2.add(Dense(n_classes))\n",
    "modelo_2.add(Activation('softmax'))\n",
    "modelo_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_2.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "102/102 [==============================] - 21s 204ms/step - loss: 3.2545 - acc: 0.0435 - val_loss: 3.1568 - val_acc: 0.0821\n",
      "Epoch 2/50\n",
      "102/102 [==============================] - 18s 180ms/step - loss: 3.1626 - acc: 0.0623 - val_loss: 3.1284 - val_acc: 0.0779\n",
      "Epoch 3/50\n",
      "102/102 [==============================] - 18s 172ms/step - loss: 3.1217 - acc: 0.0729 - val_loss: 3.0325 - val_acc: 0.1284\n",
      "Epoch 4/50\n",
      "102/102 [==============================] - 18s 179ms/step - loss: 3.0378 - acc: 0.0917 - val_loss: 2.9179 - val_acc: 0.1242\n",
      "Epoch 5/50\n",
      "102/102 [==============================] - 19s 185ms/step - loss: 2.9801 - acc: 0.1198 - val_loss: 2.8504 - val_acc: 0.1663\n",
      "Epoch 6/50\n",
      "102/102 [==============================] - 18s 175ms/step - loss: 2.9067 - acc: 0.1354 - val_loss: 2.7751 - val_acc: 0.1832\n",
      "Epoch 7/50\n",
      "102/102 [==============================] - 18s 175ms/step - loss: 2.8399 - acc: 0.1532 - val_loss: 2.7013 - val_acc: 0.2168\n",
      "Epoch 8/50\n",
      "102/102 [==============================] - 18s 180ms/step - loss: 2.8177 - acc: 0.1565 - val_loss: 2.9698 - val_acc: 0.1558\n",
      "Epoch 9/50\n",
      "102/102 [==============================] - 18s 181ms/step - loss: 2.7227 - acc: 0.1972 - val_loss: 2.5644 - val_acc: 0.2484\n",
      "Epoch 10/50\n",
      "102/102 [==============================] - 19s 189ms/step - loss: 2.6479 - acc: 0.2078 - val_loss: 2.5077 - val_acc: 0.2632\n",
      "Epoch 11/50\n",
      "102/102 [==============================] - 18s 175ms/step - loss: 2.6193 - acc: 0.2134 - val_loss: 2.4525 - val_acc: 0.2653\n",
      "Epoch 12/50\n",
      "102/102 [==============================] - 18s 178ms/step - loss: 2.5588 - acc: 0.2322 - val_loss: 2.4394 - val_acc: 0.2737\n",
      "Epoch 13/50\n",
      "102/102 [==============================] - 18s 174ms/step - loss: 2.5075 - acc: 0.2336 - val_loss: 2.3992 - val_acc: 0.2800\n",
      "Epoch 14/50\n",
      "102/102 [==============================] - 18s 180ms/step - loss: 2.4409 - acc: 0.2565 - val_loss: 2.4653 - val_acc: 0.2716\n",
      "Epoch 15/50\n",
      "102/102 [==============================] - 18s 176ms/step - loss: 2.3918 - acc: 0.2683 - val_loss: 2.2953 - val_acc: 0.3158\n",
      "Epoch 16/50\n",
      "102/102 [==============================] - 18s 180ms/step - loss: 2.3210 - acc: 0.2924 - val_loss: 2.4407 - val_acc: 0.2674\n",
      "Epoch 17/50\n",
      "102/102 [==============================] - 18s 178ms/step - loss: 2.2964 - acc: 0.2945 - val_loss: 2.4511 - val_acc: 0.2589\n",
      "Epoch 18/50\n",
      "102/102 [==============================] - 18s 175ms/step - loss: 2.2355 - acc: 0.3209 - val_loss: 2.2025 - val_acc: 0.3221\n",
      "Epoch 19/50\n",
      "102/102 [==============================] - 19s 188ms/step - loss: 2.2000 - acc: 0.3391 - val_loss: 2.3480 - val_acc: 0.2863\n",
      "Epoch 20/50\n",
      "102/102 [==============================] - 18s 176ms/step - loss: 2.1618 - acc: 0.3411 - val_loss: 2.2759 - val_acc: 0.3305\n",
      "Epoch 21/50\n",
      "102/102 [==============================] - 18s 176ms/step - loss: 2.1252 - acc: 0.3420 - val_loss: 2.2401 - val_acc: 0.3621\n",
      "Epoch 22/50\n",
      "102/102 [==============================] - 18s 178ms/step - loss: 2.1241 - acc: 0.3540 - val_loss: 2.2292 - val_acc: 0.3558\n",
      "Epoch 23/50\n",
      "102/102 [==============================] - 18s 179ms/step - loss: 2.0319 - acc: 0.3749 - val_loss: 2.2763 - val_acc: 0.3516\n",
      "Epoch 24/50\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 1.9942 - acc: 0.3804 - val_loss: 2.4111 - val_acc: 0.3179\n",
      "Epoch 25/50\n",
      "102/102 [==============================] - 19s 184ms/step - loss: 1.9613 - acc: 0.4027 - val_loss: 2.2089 - val_acc: 0.3474\n",
      "Epoch 26/50\n",
      "102/102 [==============================] - 18s 179ms/step - loss: 1.8961 - acc: 0.4178 - val_loss: 2.3845 - val_acc: 0.3221\n",
      "Epoch 27/50\n",
      "102/102 [==============================] - 19s 182ms/step - loss: 1.9196 - acc: 0.3898 - val_loss: 2.3483 - val_acc: 0.3137\n",
      "Epoch 28/50\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 1.9054 - acc: 0.4216 - val_loss: 2.2187 - val_acc: 0.3811\n",
      "Epoch 29/50\n",
      "102/102 [==============================] - 19s 184ms/step - loss: 1.8414 - acc: 0.4376 - val_loss: 2.7107 - val_acc: 0.2779\n",
      "Epoch 30/50\n",
      " 28/102 [=======>......................] - ETA: 9s - loss: 1.8226 - acc: 0.4586"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25,64,75,75] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_17/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_25/Relu, max_pooling2d_17/MaxPool, training_3/RMSprop/gradients/conv2d_18/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-6326003604d5>\", line 18, in <module>\n    validation_steps=484//batch_size,\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\models.py\", line 1227, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\", line 2016, in fit_generator\n    self._make_train_function()\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\optimizers.py\", line 225, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 583, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3755, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_17/MaxPool', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-24db1011c97e>\", line 6, in <module>\n    modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2043, in max_pool\n    name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3459, in _max_pool\n    data_format=data_format, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25,64,75,75] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_17/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_25/Relu, max_pooling2d_17/MaxPool, training_3/RMSprop/gradients/conv2d_18/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25,64,75,75] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_17/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_25/Relu, max_pooling2d_17/MaxPool, training_3/RMSprop/gradients/conv2d_18/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6326003604d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m484\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m#callbacks=[lrate]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         )\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25,64,75,75] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_17/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_25/Relu, max_pooling2d_17/MaxPool, training_3/RMSprop/gradients/conv2d_18/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-6326003604d5>\", line 18, in <module>\n    validation_steps=484//batch_size,\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\models.py\", line 1227, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\", line 2016, in fit_generator\n    self._make_train_function()\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\optimizers.py\", line 225, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 583, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3755, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_17/MaxPool', defined at:\n  File \"c:\\program files\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-24db1011c97e>\", line 6, in <module>\n    modelo_2.add(MaxPooling2D(pool_size=(2, 2)))\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2043, in max_pool\n    name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3459, in _max_pool\n    data_format=data_format, name=name)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25,64,75,75] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_3/RMSprop/gradients/max_pooling2d_17/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_17/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_25/Relu, max_pooling2d_17/MaxPool, training_3/RMSprop/gradients/conv2d_18/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_2.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_2.save_weights('modelo_2_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                24600     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 2,382,936\n",
      "Trainable params: 2,382,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic_dimension = 64\n",
    "\n",
    "modelo_3 = Sequential()\n",
    "modelo_3.add(Conv2D(64, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_3.add(Activation('relu'))\n",
    "modelo_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_3.add(Activation('relu'))\n",
    "modelo_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_3.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_3.add(Activation('relu'))\n",
    "modelo_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_3.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_3.add(Activation('relu'))\n",
    "modelo_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_3.add(Flatten())\n",
    "modelo_3.add(Dense(1024))\n",
    "modelo_3.add(Activation('relu'))\n",
    "modelo_3.add(Dropout(0.3))\n",
    "modelo_3.add(Dense(n_classes))\n",
    "modelo_3.add(Activation('softmax'))\n",
    "modelo_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_3.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 90\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 11s 410ms/step - loss: 3.2282 - acc: 0.0425 - val_loss: 3.1751 - val_acc: 0.0533\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 9s 328ms/step - loss: 3.1745 - acc: 0.0553 - val_loss: 3.1719 - val_acc: 0.0667\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 9s 317ms/step - loss: 3.2341 - acc: 0.0509 - val_loss: 3.1676 - val_acc: 0.0533\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 9s 329ms/step - loss: 3.1204 - acc: 0.0734 - val_loss: 3.0915 - val_acc: 0.1133\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 9s 326ms/step - loss: 3.0708 - acc: 0.0936 - val_loss: 3.0214 - val_acc: 0.1156\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 10s 344ms/step - loss: 3.0160 - acc: 0.1128 - val_loss: 2.8536 - val_acc: 0.1667\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 9s 333ms/step - loss: 2.9730 - acc: 0.1251 - val_loss: 2.8590 - val_acc: 0.1644\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 9s 330ms/step - loss: 2.9241 - acc: 0.1362 - val_loss: 2.9141 - val_acc: 0.1489\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 9s 328ms/step - loss: 2.8797 - acc: 0.1449 - val_loss: 2.6712 - val_acc: 0.2244\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 9s 318ms/step - loss: 2.8460 - acc: 0.1463 - val_loss: 2.7659 - val_acc: 0.1822\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 10s 341ms/step - loss: 2.8182 - acc: 0.1679 - val_loss: 2.6430 - val_acc: 0.2267\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 9s 338ms/step - loss: 2.7555 - acc: 0.1700 - val_loss: 2.8161 - val_acc: 0.1889\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 9s 330ms/step - loss: 2.7085 - acc: 0.1923 - val_loss: 2.5242 - val_acc: 0.2578\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 9s 321ms/step - loss: 2.6648 - acc: 0.1977 - val_loss: 2.5520 - val_acc: 0.2378\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 9s 334ms/step - loss: 2.6663 - acc: 0.2021 - val_loss: 2.6417 - val_acc: 0.2222\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 9s 313ms/step - loss: 2.6475 - acc: 0.2077 - val_loss: 2.4662 - val_acc: 0.2556\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 9s 337ms/step - loss: 2.5868 - acc: 0.2217 - val_loss: 2.5740 - val_acc: 0.2156\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 10s 340ms/step - loss: 2.5127 - acc: 0.2346 - val_loss: 2.4152 - val_acc: 0.2956\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 9s 329ms/step - loss: 2.5380 - acc: 0.2310 - val_loss: 2.6403 - val_acc: 0.2089\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 9s 318ms/step - loss: 2.5286 - acc: 0.2533 - val_loss: 2.4608 - val_acc: 0.2667\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 10s 340ms/step - loss: 2.4386 - acc: 0.2540 - val_loss: 2.3660 - val_acc: 0.3044\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 9s 325ms/step - loss: 2.4324 - acc: 0.2647 - val_loss: 2.4874 - val_acc: 0.2511\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 9s 333ms/step - loss: 2.3952 - acc: 0.2658 - val_loss: 2.3497 - val_acc: 0.3067\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 10s 367ms/step - loss: 2.3434 - acc: 0.2923 - val_loss: 2.4447 - val_acc: 0.2600\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 9s 339ms/step - loss: 2.3118 - acc: 0.2997 - val_loss: 2.4252 - val_acc: 0.3067\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 9s 322ms/step - loss: 2.2953 - acc: 0.2961 - val_loss: 2.3242 - val_acc: 0.3067\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 9s 336ms/step - loss: 2.2382 - acc: 0.3188 - val_loss: 2.3644 - val_acc: 0.2978\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 9s 323ms/step - loss: 2.2492 - acc: 0.3113 - val_loss: 2.2847 - val_acc: 0.3289\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 9s 329ms/step - loss: 2.2429 - acc: 0.3048 - val_loss: 2.3580 - val_acc: 0.2711\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 10s 346ms/step - loss: 2.1662 - acc: 0.3224 - val_loss: 2.3908 - val_acc: 0.3156\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 9s 328ms/step - loss: 2.1474 - acc: 0.3563 - val_loss: 2.2813 - val_acc: 0.3244\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 9s 327ms/step - loss: 2.0822 - acc: 0.3636 - val_loss: 2.3982 - val_acc: 0.2822\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 9s 327ms/step - loss: 2.0702 - acc: 0.3601 - val_loss: 2.2903 - val_acc: 0.3000\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 9s 322ms/step - loss: 2.0703 - acc: 0.3732 - val_loss: 2.3542 - val_acc: 0.2933\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 9s 328ms/step - loss: 1.9972 - acc: 0.3744 - val_loss: 2.2235 - val_acc: 0.3222\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 10s 344ms/step - loss: 2.0117 - acc: 0.3809 - val_loss: 2.2232 - val_acc: 0.3578\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 9s 326ms/step - loss: 1.9927 - acc: 0.4018 - val_loss: 2.2218 - val_acc: 0.3444\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 9s 327ms/step - loss: 1.9526 - acc: 0.3958 - val_loss: 2.3416 - val_acc: 0.3267\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 9s 321ms/step - loss: 1.9105 - acc: 0.4051 - val_loss: 2.2722 - val_acc: 0.3022\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 9s 331ms/step - loss: 1.8185 - acc: 0.4306 - val_loss: 2.4191 - val_acc: 0.3000\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 9s 330ms/step - loss: 1.8416 - acc: 0.4243 - val_loss: 2.4924 - val_acc: 0.2978\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 10s 344ms/step - loss: 1.8107 - acc: 0.4326 - val_loss: 2.3780 - val_acc: 0.3022\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 9s 333ms/step - loss: 1.7792 - acc: 0.4451 - val_loss: 2.3861 - val_acc: 0.3378\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 9s 336ms/step - loss: 1.7087 - acc: 0.4608 - val_loss: 2.4762 - val_acc: 0.3156\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 9s 324ms/step - loss: 1.7819 - acc: 0.4323 - val_loss: 2.4670 - val_acc: 0.3022\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 10s 344ms/step - loss: 1.6653 - acc: 0.4722 - val_loss: 2.3881 - val_acc: 0.3044\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 9s 313ms/step - loss: 1.6220 - acc: 0.4945 - val_loss: 2.4104 - val_acc: 0.3356\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 10s 352ms/step - loss: 1.6289 - acc: 0.4880 - val_loss: 2.2688 - val_acc: 0.3489\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 9s 329ms/step - loss: 1.6230 - acc: 0.4882 - val_loss: 2.3428 - val_acc: 0.3200\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 9s 327ms/step - loss: 1.5354 - acc: 0.5076 - val_loss: 2.6690 - val_acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_3.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_3.save_weights('modelo_3_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 24)                6168      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 13,243,224\n",
      "Trainable params: 13,243,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic_dimension = 224\n",
    "\n",
    "modelo_4 = Sequential()\n",
    "modelo_4.add(Conv2D(64, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_4.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_4.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_4.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_4.add(Flatten())\n",
    "modelo_4.add(Dense(512))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(Dropout(0.3))\n",
    "modelo_4.add(Dense(256))\n",
    "modelo_4.add(Activation('relu'))\n",
    "modelo_4.add(Dropout(0.3))\n",
    "modelo_4.add(Dense(n_classes))\n",
    "modelo_4.add(Activation('softmax'))\n",
    "modelo_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_4.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 3.2327 - acc: 0.0444 - val_loss: 3.1808 - val_acc: 0.0333\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 3.1831 - acc: 0.0506 - val_loss: 3.1495 - val_acc: 0.0688\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 32s 125ms/step - loss: 3.1485 - acc: 0.0626 - val_loss: 3.0543 - val_acc: 0.0896\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 34s 131ms/step - loss: 3.0666 - acc: 0.0783 - val_loss: 3.0267 - val_acc: 0.0625\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 3.0291 - acc: 0.0837 - val_loss: 3.0702 - val_acc: 0.0813\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 34s 131ms/step - loss: 2.9942 - acc: 0.0895 - val_loss: 2.8566 - val_acc: 0.1313\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 32s 125ms/step - loss: 2.9355 - acc: 0.1082 - val_loss: 2.7979 - val_acc: 0.1438\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 2.8847 - acc: 0.1103 - val_loss: 2.7744 - val_acc: 0.1583\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.8586 - acc: 0.1160 - val_loss: 2.7056 - val_acc: 0.1813\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.8203 - acc: 0.1266 - val_loss: 2.8600 - val_acc: 0.1229\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 34s 132ms/step - loss: 2.8218 - acc: 0.1377 - val_loss: 2.7190 - val_acc: 0.1854\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 33s 130ms/step - loss: 2.7833 - acc: 0.1472 - val_loss: 2.8168 - val_acc: 0.1542\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 34s 132ms/step - loss: 2.7688 - acc: 0.1570 - val_loss: 2.6342 - val_acc: 0.2021\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 35s 134ms/step - loss: 2.7492 - acc: 0.1674 - val_loss: 2.6633 - val_acc: 0.1875\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.7256 - acc: 0.1713 - val_loss: 2.5722 - val_acc: 0.2000\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.6931 - acc: 0.1790 - val_loss: 2.6927 - val_acc: 0.1667\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 34s 132ms/step - loss: 2.7031 - acc: 0.1767 - val_loss: 2.5664 - val_acc: 0.2146\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 2.6822 - acc: 0.1787 - val_loss: 2.5594 - val_acc: 0.2167\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.6762 - acc: 0.1934 - val_loss: 2.5379 - val_acc: 0.2521\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.6195 - acc: 0.2000 - val_loss: 2.5982 - val_acc: 0.2375\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 33s 130ms/step - loss: 2.5963 - acc: 0.2013 - val_loss: 2.4964 - val_acc: 0.2500\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 34s 132ms/step - loss: 2.6176 - acc: 0.2117 - val_loss: 2.5119 - val_acc: 0.2500\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5637 - acc: 0.2218 - val_loss: 2.4463 - val_acc: 0.2563\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.6085 - acc: 0.2083 - val_loss: 2.5982 - val_acc: 0.2313\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5920 - acc: 0.2172 - val_loss: 2.4166 - val_acc: 0.2854\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 34s 131ms/step - loss: 2.5430 - acc: 0.2134 - val_loss: 2.6197 - val_acc: 0.2188\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 2.5699 - acc: 0.2282 - val_loss: 2.4345 - val_acc: 0.2521\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5357 - acc: 0.2400 - val_loss: 2.4528 - val_acc: 0.2750\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5448 - acc: 0.2253 - val_loss: 2.3742 - val_acc: 0.2833\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 2.5227 - acc: 0.2300 - val_loss: 2.5937 - val_acc: 0.2646\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 34s 134ms/step - loss: 2.5424 - acc: 0.2300 - val_loss: 2.6654 - val_acc: 0.2083\n",
      "Epoch 32/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.5299 - acc: 0.2348 - val_loss: 2.3485 - val_acc: 0.2771\n",
      "Epoch 33/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.5062 - acc: 0.2466 - val_loss: 2.4062 - val_acc: 0.2500\n",
      "Epoch 34/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5327 - acc: 0.2218 - val_loss: 2.4917 - val_acc: 0.2750\n",
      "Epoch 35/50\n",
      "257/257 [==============================] - 34s 132ms/step - loss: 2.5038 - acc: 0.2472 - val_loss: 2.3818 - val_acc: 0.2917\n",
      "Epoch 36/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 2.5290 - acc: 0.2436 - val_loss: 2.8361 - val_acc: 0.2521\n",
      "Epoch 37/50\n",
      "257/257 [==============================] - 32s 123ms/step - loss: 2.4864 - acc: 0.2414 - val_loss: 2.3906 - val_acc: 0.2750\n",
      "Epoch 38/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.5206 - acc: 0.2366 - val_loss: 2.5155 - val_acc: 0.2875\n",
      "Epoch 39/50\n",
      "257/257 [==============================] - 34s 131ms/step - loss: 2.5296 - acc: 0.2544 - val_loss: 2.4990 - val_acc: 0.2438\n",
      "Epoch 40/50\n",
      "257/257 [==============================] - 34s 134ms/step - loss: 2.4887 - acc: 0.2413 - val_loss: 2.3895 - val_acc: 0.2917\n",
      "Epoch 41/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.5297 - acc: 0.2425 - val_loss: 2.5278 - val_acc: 0.2375\n",
      "Epoch 42/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.5457 - acc: 0.2348 - val_loss: 2.4509 - val_acc: 0.2688\n",
      "Epoch 43/50\n",
      "257/257 [==============================] - 33s 130ms/step - loss: 2.4816 - acc: 0.2527 - val_loss: 2.5877 - val_acc: 0.2667\n",
      "Epoch 44/50\n",
      "257/257 [==============================] - 34s 134ms/step - loss: 2.4822 - acc: 0.2504 - val_loss: 2.8755 - val_acc: 0.2417\n",
      "Epoch 45/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.4832 - acc: 0.2592 - val_loss: 2.7250 - val_acc: 0.3000\n",
      "Epoch 46/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.4821 - acc: 0.2550 - val_loss: 2.3465 - val_acc: 0.3333\n",
      "Epoch 47/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 2.4860 - acc: 0.2495 - val_loss: 2.4004 - val_acc: 0.2750\n",
      "Epoch 48/50\n",
      "257/257 [==============================] - 35s 135ms/step - loss: 2.4895 - acc: 0.2581 - val_loss: 2.5879 - val_acc: 0.2396\n",
      "Epoch 49/50\n",
      "257/257 [==============================] - 33s 128ms/step - loss: 2.4649 - acc: 0.2631 - val_loss: 2.5445 - val_acc: 0.2542\n",
      "Epoch 50/50\n",
      "257/257 [==============================] - 33s 129ms/step - loss: 2.4479 - acc: 0.2600 - val_loss: 2.9905 - val_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_4.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_4.save_weights('modelo_4_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_119 (Conv2D)          (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_112 (MaxPoolin (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_113 (MaxPoolin (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_114 (MaxPoolin (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_115 (MaxPoolin (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 512)               5308928   \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 24)                6168      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 5,706,584\n",
      "Trainable params: 5,706,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pic_dimension = 150\n",
    "\n",
    "modelo_5 = Sequential()\n",
    "modelo_5.add(Conv2D(64, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_5.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_5.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_5.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_5.add(Flatten())\n",
    "modelo_5.add(Dense(512))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(Dropout(0.3))\n",
    "modelo_5.add(Dense(256))\n",
    "modelo_5.add(Activation('relu'))\n",
    "modelo_5.add(Dropout(0.3))\n",
    "modelo_5.add(Dense(n_classes))\n",
    "modelo_5.add(Activation('softmax'))\n",
    "modelo_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_5.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "42/80 [==============>...............] - ETA: 17s - loss: 3.2957 - acc: 0.0424"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-55b683fd0b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m484\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m#callbacks=[lrate]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         )\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_5.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_5.save_weights('modelo_5_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_153 (Conv2D)          (None, 75, 75, 16)        1744      \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 75, 75, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_146 (MaxPoolin (None, 37, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 37, 37, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 37, 37, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_147 (MaxPoolin (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_155 (Conv2D)          (None, 18, 18, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 18, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_148 (MaxPoolin (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_156 (Conv2D)          (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_149 (MaxPoolin (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_157 (Conv2D)          (None, 4, 4, 86)          99158     \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 4, 4, 86)          344       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_150 (MaxPoolin (None, 2, 2, 86)          0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 2, 2, 86)          0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 344)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 256)               88320     \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 24)                6168      \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 361,526\n",
      "Trainable params: 359,850\n",
      "Non-trainable params: 1,676\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "pic_dimension = 150\n",
    "\n",
    "modelo_6 = Sequential()\n",
    "modelo_6.add(Conv2D(16, (6, 6), strides=(2, 2), activation='relu', padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_6.add(Dropout(0.2))\n",
    "\n",
    "modelo_6.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_6.add(Dropout(0.2))\n",
    "\n",
    "modelo_6.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_6.add(Dropout(0.2))\n",
    "\n",
    "modelo_6.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_6.add(Dropout(0.2))\n",
    "\n",
    "modelo_6.add(Conv2D(86, (3, 3), activation='relu', padding='same'))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelo_6.add(Dropout(0.2))\n",
    "\n",
    "modelo_6.add(Flatten())\n",
    "modelo_6.add(Dense(256))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(Activation('relu'))\n",
    "modelo_6.add(Dropout(0.5))\n",
    "\n",
    "modelo_6.add(Dense(256))\n",
    "modelo_6.add(BatchNormalization())\n",
    "modelo_6.add(Activation('relu'))\n",
    "modelo_6.add(Dropout(0.5))\n",
    "\n",
    "modelo_6.add(Dense(n_classes))\n",
    "modelo_6.add(Activation('softmax'))\n",
    "modelo_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_6.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 [==============================] - 38s 371ms/step - loss: 3.7793 - acc: 0.0467 - val_loss: 3.7989 - val_acc: 0.0379\n",
      "Epoch 2/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 3.5460 - acc: 0.0587 - val_loss: 3.4730 - val_acc: 0.0568\n",
      "Epoch 3/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 3.4215 - acc: 0.0736 - val_loss: 3.9356 - val_acc: 0.0442\n",
      "Epoch 4/100\n",
      "102/102 [==============================] - 22s 217ms/step - loss: 3.3006 - acc: 0.0893 - val_loss: 3.0818 - val_acc: 0.1221\n",
      "Epoch 5/100\n",
      "102/102 [==============================] - 22s 214ms/step - loss: 3.2217 - acc: 0.0897 - val_loss: 3.1207 - val_acc: 0.1347\n",
      "Epoch 6/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 3.1221 - acc: 0.1005 - val_loss: 2.9058 - val_acc: 0.1684\n",
      "Epoch 7/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 3.0646 - acc: 0.1144 - val_loss: 3.0585 - val_acc: 0.1221\n",
      "Epoch 8/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.9545 - acc: 0.1373 - val_loss: 2.7776 - val_acc: 0.1516\n",
      "Epoch 9/100\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.9463 - acc: 0.1272 - val_loss: 2.7097 - val_acc: 0.1747\n",
      "Epoch 10/100\n",
      "102/102 [==============================] - 22s 219ms/step - loss: 2.9115 - acc: 0.1438 - val_loss: 3.2526 - val_acc: 0.0884\n",
      "Epoch 11/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 2.8456 - acc: 0.1485 - val_loss: 2.8897 - val_acc: 0.1347\n",
      "Epoch 12/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 2.7966 - acc: 0.1702 - val_loss: 3.8747 - val_acc: 0.1242\n",
      "Epoch 13/100\n",
      "102/102 [==============================] - 20s 200ms/step - loss: 2.7538 - acc: 0.1823 - val_loss: 2.7144 - val_acc: 0.2021\n",
      "Epoch 14/100\n",
      "102/102 [==============================] - 21s 204ms/step - loss: 2.7112 - acc: 0.1801 - val_loss: 2.8583 - val_acc: 0.1221\n",
      "Epoch 15/100\n",
      "102/102 [==============================] - 22s 215ms/step - loss: 2.6601 - acc: 0.1985 - val_loss: 2.5701 - val_acc: 0.2274\n",
      "Epoch 16/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 2.6554 - acc: 0.2006 - val_loss: 2.5678 - val_acc: 0.2253\n",
      "Epoch 17/100\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 2.5945 - acc: 0.2067 - val_loss: 2.9970 - val_acc: 0.1537\n",
      "Epoch 18/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.6250 - acc: 0.1990 - val_loss: 2.5698 - val_acc: 0.2295\n",
      "Epoch 19/100\n",
      "102/102 [==============================] - 22s 216ms/step - loss: 2.5674 - acc: 0.1997 - val_loss: 2.8722 - val_acc: 0.2021\n",
      "Epoch 20/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.5356 - acc: 0.2210 - val_loss: 2.5572 - val_acc: 0.2126\n",
      "Epoch 21/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.5499 - acc: 0.2280 - val_loss: 2.6908 - val_acc: 0.2147\n",
      "Epoch 22/100\n",
      "102/102 [==============================] - 21s 204ms/step - loss: 2.4876 - acc: 0.2346 - val_loss: 2.7567 - val_acc: 0.1600\n",
      "Epoch 23/100\n",
      "102/102 [==============================] - 21s 208ms/step - loss: 2.5021 - acc: 0.2313 - val_loss: 3.3234 - val_acc: 0.1895\n",
      "Epoch 24/100\n",
      "102/102 [==============================] - 23s 221ms/step - loss: 2.4858 - acc: 0.2427 - val_loss: 2.4553 - val_acc: 0.2926\n",
      "Epoch 25/100\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.4408 - acc: 0.2486 - val_loss: 2.8596 - val_acc: 0.2568\n",
      "Epoch 26/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 2.4987 - acc: 0.2295 - val_loss: 3.9648 - val_acc: 0.1621\n",
      "Epoch 27/100\n",
      "102/102 [==============================] - 21s 208ms/step - loss: 2.4393 - acc: 0.2505 - val_loss: 2.8066 - val_acc: 0.2168\n",
      "Epoch 28/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.4152 - acc: 0.2480 - val_loss: 2.4175 - val_acc: 0.2611\n",
      "Epoch 29/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.3846 - acc: 0.2645 - val_loss: 2.9187 - val_acc: 0.1663\n",
      "Epoch 30/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.3667 - acc: 0.2667 - val_loss: 5.8801 - val_acc: 0.1768\n",
      "Epoch 31/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 2.3962 - acc: 0.2646 - val_loss: 2.6899 - val_acc: 0.2589\n",
      "Epoch 32/100\n",
      "102/102 [==============================] - 21s 208ms/step - loss: 2.3840 - acc: 0.2629 - val_loss: 2.7298 - val_acc: 0.1811\n",
      "Epoch 33/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.3697 - acc: 0.2696 - val_loss: 2.3485 - val_acc: 0.2484\n",
      "Epoch 34/100\n",
      "102/102 [==============================] - 21s 208ms/step - loss: 2.3526 - acc: 0.2919 - val_loss: 2.3521 - val_acc: 0.2547\n",
      "Epoch 35/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.3202 - acc: 0.2705 - val_loss: 2.5579 - val_acc: 0.2463\n",
      "Epoch 36/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 2.3274 - acc: 0.2775 - val_loss: 2.5089 - val_acc: 0.2674\n",
      "Epoch 37/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.2992 - acc: 0.2870 - val_loss: 2.3506 - val_acc: 0.2905\n",
      "Epoch 38/100\n",
      "102/102 [==============================] - 22s 218ms/step - loss: 2.2837 - acc: 0.2881 - val_loss: 2.8899 - val_acc: 0.2611\n",
      "Epoch 39/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 2.2934 - acc: 0.3012 - val_loss: 3.7726 - val_acc: 0.1811\n",
      "Epoch 40/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.2601 - acc: 0.3018 - val_loss: 2.4735 - val_acc: 0.2632\n",
      "Epoch 41/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 2.2528 - acc: 0.2996 - val_loss: 2.8102 - val_acc: 0.2295\n",
      "Epoch 42/100\n",
      "102/102 [==============================] - 21s 211ms/step - loss: 2.2331 - acc: 0.3089 - val_loss: 2.3143 - val_acc: 0.3200\n",
      "Epoch 43/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 2.2110 - acc: 0.3104 - val_loss: 2.2618 - val_acc: 0.3032\n",
      "Epoch 44/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 2.2170 - acc: 0.3125 - val_loss: 3.2645 - val_acc: 0.2842\n",
      "Epoch 45/100\n",
      "102/102 [==============================] - 20s 201ms/step - loss: 2.2194 - acc: 0.3055 - val_loss: 3.1935 - val_acc: 0.2674\n",
      "Epoch 46/100\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 2.1778 - acc: 0.3086 - val_loss: 2.2253 - val_acc: 0.3116\n",
      "Epoch 47/100\n",
      "102/102 [==============================] - 19s 190ms/step - loss: 2.1536 - acc: 0.3100 - val_loss: 2.2747 - val_acc: 0.3074\n",
      "Epoch 48/100\n",
      "102/102 [==============================] - 20s 195ms/step - loss: 2.1948 - acc: 0.3255 - val_loss: 2.7241 - val_acc: 0.1979\n",
      "Epoch 49/100\n",
      "102/102 [==============================] - 20s 194ms/step - loss: 2.1392 - acc: 0.3359 - val_loss: 2.1930 - val_acc: 0.3474\n",
      "Epoch 50/100\n",
      "102/102 [==============================] - 20s 195ms/step - loss: 2.1594 - acc: 0.3384 - val_loss: 2.1270 - val_acc: 0.3221\n",
      "Epoch 51/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 2.0982 - acc: 0.3373 - val_loss: 2.0431 - val_acc: 0.3516\n",
      "Epoch 52/100\n",
      "102/102 [==============================] - 21s 204ms/step - loss: 2.1818 - acc: 0.3204 - val_loss: 2.7000 - val_acc: 0.2421\n",
      "Epoch 53/100\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 2.1026 - acc: 0.3384 - val_loss: 3.0428 - val_acc: 0.2695\n",
      "Epoch 54/100\n",
      "102/102 [==============================] - 21s 203ms/step - loss: 2.1877 - acc: 0.3204 - val_loss: 2.7834 - val_acc: 0.2737\n",
      "Epoch 55/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.0611 - acc: 0.3313 - val_loss: 2.0141 - val_acc: 0.3579\n",
      "Epoch 56/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0517 - acc: 0.3526 - val_loss: 2.3131 - val_acc: 0.3326\n",
      "Epoch 57/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0736 - acc: 0.3393 - val_loss: 2.3375 - val_acc: 0.3137\n",
      "Epoch 58/100\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 2.0954 - acc: 0.3529 - val_loss: 3.1532 - val_acc: 0.2989\n",
      "Epoch 59/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 2.0488 - acc: 0.3597 - val_loss: 3.2547 - val_acc: 0.2926\n",
      "Epoch 60/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 2.0360 - acc: 0.3618 - val_loss: 2.5146 - val_acc: 0.3032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 2.0598 - acc: 0.3597 - val_loss: 2.4990 - val_acc: 0.3726\n",
      "Epoch 62/100\n",
      "102/102 [==============================] - 20s 200ms/step - loss: 2.0517 - acc: 0.3527 - val_loss: 2.2466 - val_acc: 0.3263\n",
      "Epoch 63/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 1.9814 - acc: 0.3690 - val_loss: 5.8737 - val_acc: 0.1916\n",
      "Epoch 64/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.0143 - acc: 0.3571 - val_loss: 1.9481 - val_acc: 0.3916\n",
      "Epoch 65/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.0607 - acc: 0.3726 - val_loss: 2.8543 - val_acc: 0.2589\n",
      "Epoch 66/100\n",
      "102/102 [==============================] - 22s 217ms/step - loss: 1.9905 - acc: 0.3899 - val_loss: 3.0869 - val_acc: 0.2505\n",
      "Epoch 67/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 2.0752 - acc: 0.3541 - val_loss: 2.2048 - val_acc: 0.3768\n",
      "Epoch 68/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 1.9628 - acc: 0.3824 - val_loss: 2.7353 - val_acc: 0.2505\n",
      "Epoch 69/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 1.9947 - acc: 0.3773 - val_loss: 2.4208 - val_acc: 0.3537\n",
      "Epoch 70/100\n",
      "102/102 [==============================] - 21s 210ms/step - loss: 1.9753 - acc: 0.3844 - val_loss: 2.1326 - val_acc: 0.3747\n",
      "Epoch 71/100\n",
      "102/102 [==============================] - 21s 205ms/step - loss: 1.9715 - acc: 0.3780 - val_loss: 2.3637 - val_acc: 0.3158\n",
      "Epoch 72/100\n",
      "102/102 [==============================] - 22s 212ms/step - loss: 1.9502 - acc: 0.4012 - val_loss: 1.9687 - val_acc: 0.3600\n",
      "Epoch 73/100\n",
      "102/102 [==============================] - 21s 208ms/step - loss: 1.9298 - acc: 0.4017 - val_loss: 2.0404 - val_acc: 0.4000\n",
      "Epoch 74/100\n",
      "102/102 [==============================] - 21s 202ms/step - loss: 1.9288 - acc: 0.3859 - val_loss: 3.2408 - val_acc: 0.2947\n",
      "Epoch 75/100\n",
      "102/102 [==============================] - 21s 204ms/step - loss: 1.9410 - acc: 0.3883 - val_loss: 3.0022 - val_acc: 0.2421\n",
      "Epoch 76/100\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8503 - acc: 0.3942 - val_loss: 2.0178 - val_acc: 0.3600\n",
      "Epoch 77/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 1.9376 - acc: 0.3948 - val_loss: 2.2016 - val_acc: 0.3242\n",
      "Epoch 78/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 1.8513 - acc: 0.4195 - val_loss: 2.8035 - val_acc: 0.2358\n",
      "Epoch 79/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 1.8802 - acc: 0.4124 - val_loss: 2.9906 - val_acc: 0.3032\n",
      "Epoch 80/100\n",
      "102/102 [==============================] - 22s 220ms/step - loss: 1.9127 - acc: 0.3998 - val_loss: 1.9306 - val_acc: 0.4337\n",
      "Epoch 81/100\n",
      "102/102 [==============================] - 22s 213ms/step - loss: 1.8656 - acc: 0.4087 - val_loss: 3.0397 - val_acc: 0.2989\n",
      "Epoch 82/100\n",
      "102/102 [==============================] - 22s 211ms/step - loss: 1.8757 - acc: 0.4055 - val_loss: 2.2102 - val_acc: 0.3453\n",
      "Epoch 83/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 1.9062 - acc: 0.4110 - val_loss: 2.0586 - val_acc: 0.3789\n",
      "Epoch 84/100\n",
      "102/102 [==============================] - 21s 206ms/step - loss: 1.8400 - acc: 0.4265 - val_loss: 2.2215 - val_acc: 0.3453\n",
      "Epoch 85/100\n",
      "102/102 [==============================] - 21s 209ms/step - loss: 1.8720 - acc: 0.4067 - val_loss: 2.0713 - val_acc: 0.3979\n",
      "Epoch 86/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 1.8056 - acc: 0.4283 - val_loss: 2.3544 - val_acc: 0.3684\n",
      "Epoch 87/100\n",
      "102/102 [==============================] - 21s 207ms/step - loss: 1.8122 - acc: 0.4352 - val_loss: 2.0131 - val_acc: 0.3558\n",
      "Epoch 88/100\n",
      "102/102 [==============================] - 21s 203ms/step - loss: 1.8434 - acc: 0.4307 - val_loss: 2.5571 - val_acc: 0.3621\n",
      "Epoch 89/100\n",
      "102/102 [==============================] - 20s 196ms/step - loss: 1.8477 - acc: 0.4318 - val_loss: 2.6302 - val_acc: 0.3242\n",
      "Epoch 90/100\n",
      "102/102 [==============================] - 20s 196ms/step - loss: 1.8969 - acc: 0.4278 - val_loss: 1.9678 - val_acc: 0.3937\n",
      "Epoch 91/100\n",
      "102/102 [==============================] - 20s 199ms/step - loss: 1.7996 - acc: 0.4252 - val_loss: 2.0611 - val_acc: 0.3768\n",
      "Epoch 92/100\n",
      "102/102 [==============================] - 20s 195ms/step - loss: 1.8083 - acc: 0.4257 - val_loss: 2.3419 - val_acc: 0.3305\n",
      "Epoch 93/100\n",
      "102/102 [==============================] - 19s 189ms/step - loss: 1.8037 - acc: 0.4335 - val_loss: 2.5086 - val_acc: 0.3116\n",
      "Epoch 94/100\n",
      "102/102 [==============================] - 21s 203ms/step - loss: 1.8123 - acc: 0.4178 - val_loss: 3.9543 - val_acc: 0.2400\n",
      "Epoch 95/100\n",
      "102/102 [==============================] - 19s 185ms/step - loss: 1.7768 - acc: 0.4353 - val_loss: 3.3454 - val_acc: 0.2568\n",
      "Epoch 96/100\n",
      "102/102 [==============================] - 20s 194ms/step - loss: 1.7782 - acc: 0.4424 - val_loss: 2.1455 - val_acc: 0.3242\n",
      "Epoch 97/100\n",
      "102/102 [==============================] - 19s 187ms/step - loss: 1.7943 - acc: 0.4422 - val_loss: 2.0849 - val_acc: 0.3789\n",
      "Epoch 98/100\n",
      "102/102 [==============================] - 19s 189ms/step - loss: 1.8041 - acc: 0.4273 - val_loss: 2.3563 - val_acc: 0.3432\n",
      "Epoch 99/100\n",
      "102/102 [==============================] - 19s 189ms/step - loss: 1.7626 - acc: 0.4368 - val_loss: 3.3791 - val_acc: 0.3095\n",
      "Epoch 100/100\n",
      "102/102 [==============================] - 20s 192ms/step - loss: 1.7408 - acc: 0.4494 - val_loss: 2.8854 - val_acc: 0.2968\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_6.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=2573//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_6.save_weights('modelo_6_top_24.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De aquí en adelante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 190, 190, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 95, 95, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 95, 95, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 95, 95, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 95, 95, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 95, 95, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 95, 95, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 95, 95, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 95, 95, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 95, 95, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 48, 48, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 48, 48, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 48, 48, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 48, 48, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 48, 48, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 48, 48, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 48, 48, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 24, 24, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 24, 24, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 24, 24, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 24, 24, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 24, 24, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 24, 24, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 24, 24, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 12, 12, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 12, 12, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 12, 12, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 12, 12, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 12, 12, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 12, 12, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 12, 12, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 12, 12, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 12, 12, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 12, 12, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 12, 12, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 12, 12, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 6, 6, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 6, 6, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 6, 6, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 6, 6, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 6, 6, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 6, 6, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 6, 6, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_16  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 24)          24600     \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 24)          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 3,253,464\n",
      "Trainable params: 3,231,576\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "pic_dimension = 190\n",
    "batch_size = 8\n",
    "\n",
    "modelo_7 = MobileNet(input_shape=(pic_dimension, pic_dimension, 3), alpha=1.0, depth_multiplier=1, dropout=15e-2, include_top=True, weights=None, input_tensor=None, pooling='max', classes=n_classes)\n",
    "\n",
    "modelo_7.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy')\n",
    "\n",
    "modelo_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3183 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n",
      "Epoch 1/100\n",
      "397/397 [==============================] - 226s 570ms/step - loss: 3.3507 - val_loss: 3.4615\n",
      "Epoch 2/100\n",
      "397/397 [==============================] - 70s 178ms/step - loss: 3.1538 - val_loss: 3.6025\n",
      "Epoch 3/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 3.0183 - val_loss: 5.7055\n",
      "Epoch 4/100\n",
      "397/397 [==============================] - 72s 181ms/step - loss: 2.9206 - val_loss: 5.2644\n",
      "Epoch 5/100\n",
      "397/397 [==============================] - 72s 180ms/step - loss: 2.8517 - val_loss: 3.6040\n",
      "Epoch 6/100\n",
      "397/397 [==============================] - 74s 185ms/step - loss: 2.8041 - val_loss: 3.2154\n",
      "Epoch 7/100\n",
      "397/397 [==============================] - 155s 390ms/step - loss: 2.7598 - val_loss: 4.1690\n",
      "Epoch 8/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 2.7148 - val_loss: 2.9303\n",
      "Epoch 9/100\n",
      "397/397 [==============================] - 72s 181ms/step - loss: 2.6682 - val_loss: 2.9950\n",
      "Epoch 10/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 2.6063 - val_loss: 2.7254\n",
      "Epoch 11/100\n",
      "397/397 [==============================] - 72s 182ms/step - loss: 2.5021 - val_loss: 2.5166\n",
      "Epoch 12/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 2.4501 - val_loss: 3.2081\n",
      "Epoch 13/100\n",
      "397/397 [==============================] - 71s 180ms/step - loss: 2.4024 - val_loss: 2.6640\n",
      "Epoch 14/100\n",
      "397/397 [==============================] - 72s 181ms/step - loss: 2.3425 - val_loss: 3.0560\n",
      "Epoch 15/100\n",
      "397/397 [==============================] - 72s 180ms/step - loss: 2.2948 - val_loss: 3.3623\n",
      "Epoch 16/100\n",
      "397/397 [==============================] - 72s 181ms/step - loss: 2.2101 - val_loss: 2.2461\n",
      "Epoch 17/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 2.2073 - val_loss: 2.5772\n",
      "Epoch 18/100\n",
      "397/397 [==============================] - 72s 182ms/step - loss: 2.1046 - val_loss: 2.2581\n",
      "Epoch 19/100\n",
      "397/397 [==============================] - 73s 183ms/step - loss: 2.0617 - val_loss: 3.3650\n",
      "Epoch 20/100\n",
      "397/397 [==============================] - 73s 184ms/step - loss: 2.0663 - val_loss: 2.5787\n",
      "Epoch 21/100\n",
      "397/397 [==============================] - 72s 182ms/step - loss: 1.9864 - val_loss: 2.3402\n",
      "Epoch 22/100\n",
      "397/397 [==============================] - 72s 181ms/step - loss: 1.9472 - val_loss: 2.6801\n",
      "Epoch 23/100\n",
      "397/397 [==============================] - 72s 180ms/step - loss: 1.9201 - val_loss: 3.2647\n",
      "Epoch 24/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.9110 - val_loss: 2.0981\n",
      "Epoch 25/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.8523 - val_loss: 2.1337\n",
      "Epoch 26/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.7996 - val_loss: 2.5030\n",
      "Epoch 27/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.8148 - val_loss: 2.0430\n",
      "Epoch 28/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.7592 - val_loss: 1.8657\n",
      "Epoch 29/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.7121 - val_loss: 1.9896\n",
      "Epoch 30/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.6996 - val_loss: 2.0466\n",
      "Epoch 31/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.6841 - val_loss: 1.6228\n",
      "Epoch 32/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.6335 - val_loss: 2.1877\n",
      "Epoch 33/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.5821 - val_loss: 2.7306\n",
      "Epoch 34/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.6042 - val_loss: 2.6182\n",
      "Epoch 35/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5586 - val_loss: 2.4633\n",
      "Epoch 36/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.5222 - val_loss: 2.0251\n",
      "Epoch 37/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5035 - val_loss: 2.0995\n",
      "Epoch 38/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4889 - val_loss: 1.7392\n",
      "Epoch 39/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4438 - val_loss: 2.0304\n",
      "Epoch 40/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.4293 - val_loss: 1.4692\n",
      "Epoch 41/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.4034 - val_loss: 1.7067\n",
      "Epoch 42/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.3679 - val_loss: 1.8836\n",
      "Epoch 43/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.3676 - val_loss: 1.8735\n",
      "Epoch 44/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.3250 - val_loss: 2.0263\n",
      "Epoch 45/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.3020 - val_loss: 1.9532\n",
      "Epoch 46/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.3082 - val_loss: 1.5887\n",
      "Epoch 47/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.2552 - val_loss: 1.3501\n",
      "Epoch 48/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.2539 - val_loss: 1.3989\n",
      "Epoch 49/100\n",
      "397/397 [==============================] - 70s 178ms/step - loss: 1.2210 - val_loss: 2.3225\n",
      "Epoch 50/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.1755 - val_loss: 1.5725\n",
      "Epoch 51/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.1874 - val_loss: 1.5005\n",
      "Epoch 52/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.1678 - val_loss: 1.8881\n",
      "Epoch 53/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.1499 - val_loss: 1.5658\n",
      "Epoch 54/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 1.1339 - val_loss: 1.7542\n",
      "Epoch 55/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.0958 - val_loss: 1.5800\n",
      "Epoch 56/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.1192 - val_loss: 1.5424\n",
      "Epoch 57/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.0570 - val_loss: 1.3146\n",
      "Epoch 58/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 1.0514 - val_loss: 1.5110\n",
      "Epoch 59/100\n",
      "397/397 [==============================] - 70s 178ms/step - loss: 1.0341 - val_loss: 1.9015\n",
      "Epoch 60/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 1.0014 - val_loss: 1.6283\n",
      "Epoch 61/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.0103 - val_loss: 1.4666\n",
      "Epoch 62/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.9642 - val_loss: 1.6035\n",
      "Epoch 63/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 0.9596 - val_loss: 1.4284\n",
      "Epoch 64/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.9549 - val_loss: 1.4455\n",
      "Epoch 65/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.9592 - val_loss: 1.8027\n",
      "Epoch 66/100\n",
      "397/397 [==============================] - 70s 178ms/step - loss: 0.9503 - val_loss: 2.0399\n",
      "Epoch 67/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.9124 - val_loss: 1.3332\n",
      "Epoch 68/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.8901 - val_loss: 1.4591\n",
      "Epoch 69/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.8895 - val_loss: 1.6986\n",
      "Epoch 70/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.8572 - val_loss: 1.2256\n",
      "Epoch 71/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.8248 - val_loss: 1.3718\n",
      "Epoch 72/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 0.8496 - val_loss: 1.3971\n",
      "Epoch 73/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.8216 - val_loss: 1.3661\n",
      "Epoch 74/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.8203 - val_loss: 1.2670\n",
      "Epoch 75/100\n",
      "397/397 [==============================] - 69s 175ms/step - loss: 0.8125 - val_loss: 1.2223\n",
      "Epoch 76/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.7834 - val_loss: 1.4603\n",
      "Epoch 77/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.8051 - val_loss: 1.4743\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/397 [==============================] - 69s 175ms/step - loss: 0.7962 - val_loss: 1.3856\n",
      "Epoch 79/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.7752 - val_loss: 1.9266\n",
      "Epoch 80/100\n",
      "397/397 [==============================] - 70s 175ms/step - loss: 0.7594 - val_loss: 1.3881\n",
      "Epoch 81/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.7985 - val_loss: 1.3616\n",
      "Epoch 82/100\n",
      "397/397 [==============================] - 69s 174ms/step - loss: 0.7447 - val_loss: 1.7708\n",
      "Epoch 83/100\n",
      "397/397 [==============================] - 69s 174ms/step - loss: 0.7106 - val_loss: 1.7196\n",
      "Epoch 84/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.7124 - val_loss: 1.3534\n",
      "Epoch 85/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.7067 - val_loss: 1.1503\n",
      "Epoch 86/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.7430 - val_loss: 1.3030\n",
      "Epoch 87/100\n",
      "397/397 [==============================] - 69s 174ms/step - loss: 0.7217 - val_loss: 1.2131\n",
      "Epoch 88/100\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 0.6902 - val_loss: 1.6850\n",
      "Epoch 89/100\n",
      "397/397 [==============================] - 69s 174ms/step - loss: 0.6793 - val_loss: 1.4603\n",
      "Epoch 90/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.6712 - val_loss: 1.4904\n",
      "Epoch 91/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.6646 - val_loss: 1.6743\n",
      "Epoch 92/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.6748 - val_loss: 1.3206\n",
      "Epoch 93/100\n",
      "397/397 [==============================] - 70s 175ms/step - loss: 0.6940 - val_loss: 1.5381\n",
      "Epoch 94/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.6365 - val_loss: 1.5401\n",
      "Epoch 95/100\n",
      "397/397 [==============================] - 70s 175ms/step - loss: 0.6590 - val_loss: 1.4647\n",
      "Epoch 96/100\n",
      "397/397 [==============================] - 71s 179ms/step - loss: 0.6308 - val_loss: 1.3117\n",
      "Epoch 97/100\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 0.6257 - val_loss: 1.5842\n",
      "Epoch 98/100\n",
      "397/397 [==============================] - 69s 174ms/step - loss: 0.5983 - val_loss: 1.6628\n",
      "Epoch 99/100\n",
      "397/397 [==============================] - 70s 175ms/step - loss: 0.6188 - val_loss: 1.7617\n",
      "Epoch 100/100\n",
      "397/397 [==============================] - 70s 176ms/step - loss: 0.6057 - val_loss: 1.5711\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "results = modelo_7.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=3183//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_7.save_weights('modelo_7_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 190, 190, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 95, 95, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 95, 95, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 95, 95, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 95, 95, 24)        216       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 95, 95, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 95, 95, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 95, 95, 48)        1152      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 95, 95, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 95, 95, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 48, 48, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 48, 48, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 48, 48, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 48, 48, 96)        4608      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 48, 48, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 48, 48, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 24, 24, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 24, 24, 192)       18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 24, 24, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 24, 24, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 12, 12, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 12, 12, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 12, 12, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 12, 12, 384)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 6, 6, 384)         3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 6, 6, 768)         294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 6, 6, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 6, 6, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_19  (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 768)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 24)          18456     \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 24)          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 1,851,432\n",
      "Trainable params: 1,835,016\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "pic_dimension = 190\n",
    "batch_size = 10\n",
    "\n",
    "modelo_8 = MobileNet(input_shape=(pic_dimension, pic_dimension, 3), alpha=0.75, depth_multiplier=1, dropout=20e-2, include_top=True, weights=None, input_tensor=None, pooling='max', classes=n_classes)\n",
    "\n",
    "modelo_8.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy')\n",
    "\n",
    "modelo_8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3183 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n",
      "Epoch 1/100\n",
      "318/318 [==============================] - 110s 347ms/step - loss: 3.3144 - val_loss: 3.7511\n",
      "Epoch 2/100\n",
      "318/318 [==============================] - 72s 225ms/step - loss: 3.2249 - val_loss: 3.8049\n",
      "Epoch 3/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 3.1052 - val_loss: 4.4630\n",
      "Epoch 4/100\n",
      "318/318 [==============================] - 72s 226ms/step - loss: 2.9686 - val_loss: 3.9922\n",
      "Epoch 5/100\n",
      "318/318 [==============================] - 72s 225ms/step - loss: 2.8438 - val_loss: 4.3032\n",
      "Epoch 6/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.7657 - val_loss: 6.0578\n",
      "Epoch 7/100\n",
      "318/318 [==============================] - 72s 225ms/step - loss: 2.7176 - val_loss: 5.3667\n",
      "Epoch 8/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.6668 - val_loss: 3.3521\n",
      "Epoch 9/100\n",
      "318/318 [==============================] - 71s 225ms/step - loss: 2.5792 - val_loss: 4.2831\n",
      "Epoch 10/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.5575 - val_loss: 4.2693\n",
      "Epoch 11/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.4946 - val_loss: 3.3327\n",
      "Epoch 12/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.4399 - val_loss: 2.7317\n",
      "Epoch 13/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.3894 - val_loss: 2.8882\n",
      "Epoch 14/100\n",
      "318/318 [==============================] - 72s 225ms/step - loss: 2.3391 - val_loss: 3.8940\n",
      "Epoch 15/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 2.3106 - val_loss: 2.6180\n",
      "Epoch 16/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 2.2488 - val_loss: 2.5705\n",
      "Epoch 17/100\n",
      "318/318 [==============================] - 74s 232ms/step - loss: 2.2326 - val_loss: 2.6166 2.2\n",
      "Epoch 18/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 2.1836 - val_loss: 2.4335\n",
      "Epoch 19/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 2.1535 - val_loss: 2.5656\n",
      "Epoch 20/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 2.1404 - val_loss: 2.3746\n",
      "Epoch 21/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 2.0523 - val_loss: 2.9617\n",
      "Epoch 22/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 2.0336 - val_loss: 2.7755\n",
      "Epoch 23/100\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 1.9930 - val_loss: 2.1050\n",
      "Epoch 24/100\n",
      "318/318 [==============================] - 69s 219ms/step - loss: 1.9772 - val_loss: 2.3638\n",
      "Epoch 25/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.9269 - val_loss: 2.1173lo\n",
      "Epoch 26/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 1.9208 - val_loss: 3.1661\n",
      "Epoch 27/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 1.8782 - val_loss: 2.3663\n",
      "Epoch 28/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 1.8541 - val_loss: 2.9739\n",
      "Epoch 29/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 1.8178 - val_loss: 2.1350\n",
      "Epoch 30/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 1.7710 - val_loss: 2.5976\n",
      "Epoch 31/100\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 1.7703 - val_loss: 1.8729\n",
      "Epoch 32/100\n",
      "318/318 [==============================] - 68s 215ms/step - loss: 1.7182 - val_loss: 3.1639\n",
      "Epoch 33/100\n",
      "318/318 [==============================] - 70s 222ms/step - loss: 1.6901 - val_loss: 2.1111\n",
      "Epoch 34/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 1.6318 - val_loss: 2.0828\n",
      "Epoch 35/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 1.6467 - val_loss: 3.0301\n",
      "Epoch 36/100\n",
      "318/318 [==============================] - 72s 225ms/step - loss: 1.5970 - val_loss: 1.8854\n",
      "Epoch 37/100\n",
      "318/318 [==============================] - 72s 228ms/step - loss: 1.5824 - val_loss: 1.8971\n",
      "Epoch 38/100\n",
      "318/318 [==============================] - 72s 228ms/step - loss: 1.5933 - val_loss: 1.6244\n",
      "Epoch 39/100\n",
      "318/318 [==============================] - 71s 225ms/step - loss: 1.5538 - val_loss: 1.9469\n",
      "Epoch 40/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.4840 - val_loss: 1.8715\n",
      "Epoch 41/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.4891 - val_loss: 2.0697\n",
      "Epoch 42/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 1.4101 - val_loss: 1.9587\n",
      "Epoch 43/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.4726 - val_loss: 3.4243\n",
      "Epoch 44/100\n",
      "318/318 [==============================] - 72s 227ms/step - loss: 1.4643 - val_loss: 2.0727\n",
      "Epoch 45/100\n",
      "318/318 [==============================] - 72s 227ms/step - loss: 1.4021 - val_loss: 1.4582\n",
      "Epoch 46/100\n",
      "318/318 [==============================] - 72s 226ms/step - loss: 1.3638 - val_loss: 1.6609\n",
      "Epoch 47/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.3518 - val_loss: 2.3472\n",
      "Epoch 48/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.3619 - val_loss: 1.8124\n",
      "Epoch 49/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 1.3114 - val_loss: 1.9644\n",
      "Epoch 50/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.2899 - val_loss: 1.6285\n",
      "Epoch 51/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 1.2754 - val_loss: 1.5773\n",
      "Epoch 52/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.2504 - val_loss: 1.8580\n",
      "Epoch 53/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 1.2506 - val_loss: 1.8989\n",
      "Epoch 54/100\n",
      "318/318 [==============================] - 73s 229ms/step - loss: 1.2214 - val_loss: 1.5785\n",
      "Epoch 55/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 1.2033 - val_loss: 2.0919\n",
      "Epoch 56/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 1.2032 - val_loss: 2.1304\n",
      "Epoch 57/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 1.1500 - val_loss: 1.9243\n",
      "Epoch 58/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 1.1307 - val_loss: 1.7709\n",
      "Epoch 59/100\n",
      "318/318 [==============================] - 74s 232ms/step - loss: 1.1139 - val_loss: 2.0987\n",
      "Epoch 60/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 1.0957 - val_loss: 2.30910s - loss: 1.094\n",
      "Epoch 61/100\n",
      "318/318 [==============================] - 71s 224ms/step - loss: 1.0895 - val_loss: 1.9455\n",
      "Epoch 62/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 1.0762 - val_loss: 1.5264\n",
      "Epoch 63/100\n",
      "318/318 [==============================] - 72s 227ms/step - loss: 1.0533 - val_loss: 1.5429\n",
      "Epoch 64/100\n",
      "318/318 [==============================] - 85s 266ms/step - loss: 1.0824 - val_loss: 1.6186\n",
      "Epoch 65/100\n",
      "318/318 [==============================] - 72s 227ms/step - loss: 1.0705 - val_loss: 1.5853\n",
      "Epoch 66/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 1.0349 - val_loss: 2.0581\n",
      "Epoch 67/100\n",
      "318/318 [==============================] - 69s 218ms/step - loss: 0.9973 - val_loss: 1.5790\n",
      "Epoch 68/100\n",
      "318/318 [==============================] - 72s 226ms/step - loss: 0.9799 - val_loss: 1.9711\n",
      "Epoch 69/100\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 0.9717 - val_loss: 1.2973\n",
      "Epoch 70/100\n",
      "318/318 [==============================] - 71s 225ms/step - loss: 0.9331 - val_loss: 2.2610\n",
      "Epoch 71/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.9404 - val_loss: 1.4212\n",
      "Epoch 72/100\n",
      "318/318 [==============================] - 70s 222ms/step - loss: 0.9487 - val_loss: 2.3796\n",
      "Epoch 73/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 0.9352 - val_loss: 1.2924\n",
      "Epoch 74/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 0.9291 - val_loss: 1.8447\n",
      "Epoch 75/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.8914 - val_loss: 3.0031\n",
      "Epoch 76/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 0.8891 - val_loss: 2.5109\n",
      "Epoch 77/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.8727 - val_loss: 1.7456\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - 70s 221ms/step - loss: 0.8566 - val_loss: 1.6588\n",
      "Epoch 79/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.8797 - val_loss: 1.5365\n",
      "Epoch 80/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 0.8867 - val_loss: 1.7238\n",
      "Epoch 81/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.8239 - val_loss: 1.8672\n",
      "Epoch 82/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.8001 - val_loss: 1.5806\n",
      "Epoch 83/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 0.7792 - val_loss: 1.4480\n",
      "Epoch 84/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.7995 - val_loss: 1.9856\n",
      "Epoch 85/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 0.7785 - val_loss: 1.3694\n",
      "Epoch 86/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.8082 - val_loss: 1.4416\n",
      "Epoch 87/100\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 0.7700 - val_loss: 1.5846\n",
      "Epoch 88/100\n",
      "318/318 [==============================] - 73s 229ms/step - loss: 0.7480 - val_loss: 1.9088\n",
      "Epoch 89/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.7973 - val_loss: 1.5522\n",
      "Epoch 90/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.7428 - val_loss: 1.5113\n",
      "Epoch 91/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.7541 - val_loss: 1.5427\n",
      "Epoch 92/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 0.7508 - val_loss: 1.6525\n",
      "Epoch 93/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 0.7319 - val_loss: 1.7444\n",
      "Epoch 94/100\n",
      "318/318 [==============================] - 70s 221ms/step - loss: 0.7301 - val_loss: 1.7512\n",
      "Epoch 95/100\n",
      "318/318 [==============================] - 70s 219ms/step - loss: 0.7093 - val_loss: 2.0723\n",
      "Epoch 96/100\n",
      "318/318 [==============================] - 71s 222ms/step - loss: 0.7253 - val_loss: 1.4452\n",
      "Epoch 97/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.6898 - val_loss: 1.7279\n",
      "Epoch 98/100\n",
      "318/318 [==============================] - 71s 223ms/step - loss: 0.7059 - val_loss: 1.4715\n",
      "Epoch 99/100\n",
      "318/318 [==============================] - 69s 217ms/step - loss: 0.6689 - val_loss: 1.7920\n",
      "Epoch 100/100\n",
      "318/318 [==============================] - 70s 220ms/step - loss: 0.6786 - val_loss: 1.7409\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "results = modelo_8.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=3183//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_8.save_weights('modelo_8_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 190, 190, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 95, 95, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 95, 95, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 95, 95, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 95, 95, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 95, 95, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 95, 95, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 95, 95, 48)        2304      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 95, 95, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 95, 95, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 48, 48, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 48, 48, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 48, 48, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 48, 48, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 48, 48, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 48, 48, 96)        18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 48, 48, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 24, 24, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 24, 24, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 24, 24, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 24, 24, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 24, 24, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 24, 24, 192)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 24, 24, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 24, 24, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 12, 12, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 12, 12, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 12, 12, 768)       6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 12, 12, 768)       3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 12, 12, 768)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 12, 12, 384)       294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 12, 12, 768)       6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 12, 12, 768)       3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 12, 12, 768)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 12, 12, 384)       294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 12, 12, 768)       6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 12, 12, 768)       3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 12, 12, 768)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 12, 12, 384)       294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 12, 12, 768)       6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 12, 12, 768)       3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 12, 12, 768)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 12, 12, 384)       294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 12, 12, 768)       6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 12, 12, 768)       3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 12, 12, 768)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 12, 12, 384)       294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 6, 6, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 6, 6, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 6, 6, 1536)        13824     \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 6, 6, 1536)        6144      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 6, 6, 1536)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 6, 6, 768)         1179648   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 6, 6, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_20  (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 768)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 24)          18456     \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 24)          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 3,665,808\n",
      "Trainable params: 3,641,952\n",
      "Non-trainable params: 23,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "pic_dimension = 190\n",
    "batch_size = 10\n",
    "\n",
    "modelo_9 = MobileNet(input_shape=(pic_dimension, pic_dimension, 3), alpha=0.75, depth_multiplier=2, dropout=20e-2, include_top=True, weights=None, input_tensor=None, pooling='max', classes=n_classes)\n",
    "\n",
    "modelo_9.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy')\n",
    "\n",
    "modelo_9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3183 images belonging to 24 classes.\n",
      "Found 484 images belonging to 24 classes.\n",
      "Epoch 1/100\n",
      "318/318 [==============================] - 405s 1s/step - loss: 3.3304 - val_loss: 3.5224\n",
      "Epoch 2/100\n",
      "318/318 [==============================] - 79s 249ms/step - loss: 3.1208 - val_loss: 4.6867\n",
      "Epoch 3/100\n",
      "318/318 [==============================] - 77s 241ms/step - loss: 2.9605 - val_loss: 5.7034\n",
      "Epoch 4/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 2.8216 - val_loss: 4.0611\n",
      "Epoch 5/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 2.6987 - val_loss: 7.2515\n",
      "Epoch 6/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 2.6435 - val_loss: 4.2025\n",
      "Epoch 7/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 2.5828 - val_loss: 3.2920\n",
      "Epoch 8/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 2.5088 - val_loss: 2.9703\n",
      "Epoch 9/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 2.4569 - val_loss: 5.4913\n",
      "Epoch 10/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 2.4263 - val_loss: 2.9945\n",
      "Epoch 11/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 2.3591 - val_loss: 2.5494\n",
      "Epoch 12/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 2.2852 - val_loss: 4.3754\n",
      "Epoch 13/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 2.2411 - val_loss: 2.7517\n",
      "Epoch 14/100\n",
      "318/318 [==============================] - 76s 237ms/step - loss: 2.1857 - val_loss: 3.0140\n",
      "Epoch 15/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 2.1905 - val_loss: 2.4861\n",
      "Epoch 16/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 2.0786 - val_loss: 2.4203\n",
      "Epoch 17/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 2.0467 - val_loss: 3.4806\n",
      "Epoch 18/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 2.0163 - val_loss: 2.4770\n",
      "Epoch 19/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 1.9536 - val_loss: 2.2596\n",
      "Epoch 20/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 1.9314 - val_loss: 2.7252\n",
      "Epoch 21/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.9224 - val_loss: 2.5329\n",
      "Epoch 22/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.8120 - val_loss: 2.3824\n",
      "Epoch 23/100\n",
      "318/318 [==============================] - 74s 234ms/step - loss: 1.8066 - val_loss: 2.0115\n",
      "Epoch 24/100\n",
      "318/318 [==============================] - 74s 234ms/step - loss: 1.7979 - val_loss: 4.4516\n",
      "Epoch 25/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.7611 - val_loss: 2.1723\n",
      "Epoch 26/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.6956 - val_loss: 1.9240\n",
      "Epoch 27/100\n",
      "318/318 [==============================] - 75s 234ms/step - loss: 1.6968 - val_loss: 1.9791\n",
      "Epoch 28/100\n",
      "318/318 [==============================] - 77s 242ms/step - loss: 1.6373 - val_loss: 2.9663\n",
      "Epoch 29/100\n",
      "318/318 [==============================] - 78s 244ms/step - loss: 1.6432 - val_loss: 1.8404\n",
      "Epoch 30/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 1.5931 - val_loss: 2.2341\n",
      "Epoch 31/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 1.5678 - val_loss: 1.9845\n",
      "Epoch 32/100\n",
      "318/318 [==============================] - 74s 231ms/step - loss: 1.5469 - val_loss: 2.0945\n",
      "Epoch 33/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.5129 - val_loss: 2.6251\n",
      "Epoch 34/100\n",
      "318/318 [==============================] - 74s 233ms/step - loss: 1.5180 - val_loss: 1.9751\n",
      "Epoch 35/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.4030 - val_loss: 1.8883\n",
      "Epoch 36/100\n",
      "318/318 [==============================] - 81s 254ms/step - loss: 1.4557 - val_loss: 1.6351\n",
      "Epoch 37/100\n",
      "318/318 [==============================] - 80s 250ms/step - loss: 1.4038 - val_loss: 1.6620\n",
      "Epoch 38/100\n",
      "318/318 [==============================] - 79s 248ms/step - loss: 1.3673 - val_loss: 1.5778\n",
      "Epoch 39/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 1.3750 - val_loss: 1.8146\n",
      "Epoch 40/100\n",
      "318/318 [==============================] - 77s 242ms/step - loss: 1.3366 - val_loss: 1.7263\n",
      "Epoch 41/100\n",
      "318/318 [==============================] - 78s 246ms/step - loss: 1.2981 - val_loss: 1.4345\n",
      "Epoch 42/100\n",
      "318/318 [==============================] - 77s 241ms/step - loss: 1.2696 - val_loss: 1.9697\n",
      "Epoch 43/100\n",
      "318/318 [==============================] - 77s 243ms/step - loss: 1.2836 - val_loss: 1.6228\n",
      "Epoch 44/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 1.2206 - val_loss: 1.4572\n",
      "Epoch 45/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 1.2040 - val_loss: 2.1538\n",
      "Epoch 46/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 1.1832 - val_loss: 1.8981\n",
      "Epoch 47/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 1.1777 - val_loss: 1.7404\n",
      "Epoch 48/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 1.1258 - val_loss: 1.3815\n",
      "Epoch 49/100\n",
      "318/318 [==============================] - 77s 242ms/step - loss: 1.1169 - val_loss: 1.7339\n",
      "Epoch 50/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 1.0902 - val_loss: 1.5974\n",
      "Epoch 51/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 1.0868 - val_loss: 1.3655\n",
      "Epoch 52/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 1.0983 - val_loss: 1.6205\n",
      "Epoch 53/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 1.0649 - val_loss: 1.6966\n",
      "Epoch 54/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 1.0252 - val_loss: 2.0721\n",
      "Epoch 55/100\n",
      "318/318 [==============================] - 86s 271ms/step - loss: 1.0113 - val_loss: 2.6836\n",
      "Epoch 56/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 1.0033 - val_loss: 1.8985\n",
      "Epoch 57/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 0.9885 - val_loss: 1.5278\n",
      "Epoch 58/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.9630 - val_loss: 1.6470\n",
      "Epoch 59/100\n",
      "318/318 [==============================] - 82s 257ms/step - loss: 0.9545 - val_loss: 1.3445\n",
      "Epoch 60/100\n",
      "318/318 [==============================] - 80s 251ms/step - loss: 0.9169 - val_loss: 1.6632\n",
      "Epoch 61/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 0.9080 - val_loss: 2.2280\n",
      "Epoch 62/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.8929 - val_loss: 1.7714\n",
      "Epoch 63/100\n",
      "318/318 [==============================] - 77s 241ms/step - loss: 0.9037 - val_loss: 1.8449\n",
      "Epoch 64/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.9291 - val_loss: 1.2369\n",
      "Epoch 65/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 0.8756 - val_loss: 1.5245\n",
      "Epoch 66/100\n",
      "318/318 [==============================] - 76s 240ms/step - loss: 0.8601 - val_loss: 1.5723\n",
      "Epoch 67/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.8624 - val_loss: 1.5863\n",
      "Epoch 68/100\n",
      "318/318 [==============================] - 75s 237ms/step - loss: 0.8252 - val_loss: 1.1763\n",
      "Epoch 69/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.8198 - val_loss: 2.1124\n",
      "Epoch 70/100\n",
      "318/318 [==============================] - 75s 237ms/step - loss: 0.8480 - val_loss: 1.3384\n",
      "Epoch 71/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 0.8106 - val_loss: 1.7700\n",
      "Epoch 72/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.7844 - val_loss: 1.4954\n",
      "Epoch 73/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 0.7939 - val_loss: 1.4038\n",
      "Epoch 74/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.7737 - val_loss: 2.9905\n",
      "Epoch 75/100\n",
      "318/318 [==============================] - 76s 238ms/step - loss: 0.7496 - val_loss: 1.4432\n",
      "Epoch 76/100\n",
      "318/318 [==============================] - 75s 237ms/step - loss: 0.7468 - val_loss: 1.2893\n",
      "Epoch 77/100\n",
      "318/318 [==============================] - 76s 239ms/step - loss: 0.7329 - val_loss: 1.3232\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - 76s 238ms/step - loss: 0.7204 - val_loss: 1.4105\n",
      "Epoch 79/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 0.7226 - val_loss: 1.5546\n",
      "Epoch 80/100\n",
      "318/318 [==============================] - 75s 237ms/step - loss: 0.6968 - val_loss: 1.5845\n",
      "Epoch 81/100\n",
      "318/318 [==============================] - 78s 244ms/step - loss: 0.7006 - val_loss: 1.9927\n",
      "Epoch 82/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 0.7048 - val_loss: 1.4515\n",
      "Epoch 83/100\n",
      "318/318 [==============================] - 74s 231ms/step - loss: 0.6487 - val_loss: 1.5818\n",
      "Epoch 84/100\n",
      "318/318 [==============================] - 75s 235ms/step - loss: 0.6814 - val_loss: 2.1638\n",
      "Epoch 85/100\n",
      "318/318 [==============================] - 74s 234ms/step - loss: 0.6901 - val_loss: 1.2930\n",
      "Epoch 86/100\n",
      "318/318 [==============================] - 74s 233ms/step - loss: 0.6287 - val_loss: 1.4788ss\n",
      "Epoch 87/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 0.5942 - val_loss: 2.4726\n",
      "Epoch 88/100\n",
      "318/318 [==============================] - 74s 232ms/step - loss: 0.6617 - val_loss: 1.6930\n",
      "Epoch 89/100\n",
      "318/318 [==============================] - 74s 233ms/step - loss: 0.6519 - val_loss: 1.4895\n",
      "Epoch 90/100\n",
      "318/318 [==============================] - 74s 232ms/step - loss: 0.6269 - val_loss: 1.5541\n",
      "Epoch 91/100\n",
      "318/318 [==============================] - 75s 236ms/step - loss: 0.5979 - val_loss: 1.6360\n",
      "Epoch 92/100\n",
      "318/318 [==============================] - 74s 234ms/step - loss: 0.6459 - val_loss: 1.4302\n",
      "Epoch 93/100\n",
      "318/318 [==============================] - 74s 231ms/step - loss: 0.5891 - val_loss: 1.7486\n",
      "Epoch 94/100\n",
      "318/318 [==============================] - 74s 233ms/step - loss: 0.5989 - val_loss: 1.5326\n",
      "Epoch 95/100\n",
      "318/318 [==============================] - 75s 234ms/step - loss: 0.5994 - val_loss: 1.7299\n",
      "Epoch 96/100\n",
      "318/318 [==============================] - 74s 233ms/step - loss: 0.6015 - val_loss: 1.3271\n",
      "Epoch 97/100\n",
      "318/318 [==============================] - 73s 231ms/step - loss: 0.5627 - val_loss: 1.7541\n",
      "Epoch 98/100\n",
      "318/318 [==============================] - 75s 237ms/step - loss: 0.5682 - val_loss: 1.8493\n",
      "Epoch 99/100\n",
      "318/318 [==============================] - 74s 232ms/step - loss: 0.5605 - val_loss: 1.5153\n",
      "Epoch 100/100\n",
      "318/318 [==============================] - 74s 234ms/step - loss: 0.5575 - val_loss: 1.4890\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "results = modelo_9.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=3183//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_9.save_weights('modelo_9_top_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_9.predict_generator(validation_generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "pic_dimension = 190\n",
    "batch_size = 10\n",
    "\n",
    "modelo_10 = MobileNet(input_shape=(pic_dimension, pic_dimension, 3), alpha=0.75, depth_multiplier=2, dropout=20e-2, include_top=True, weights=None, input_tensor=None, pooling='max', classes=n_classes)\n",
    "\n",
    "modelo_10.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy')\n",
    "\n",
    "modelo_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "results = modelo_10.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=3183//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=484//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_10.save_weights('modelo_9_top_24.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se debe tener las últimas versiones de Keras y TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = True\n",
    "dog_breed_train_path = 'Dog Breed/train/'\n",
    "dog_breed_val_path = 'Dog Breed/val/'\n",
    "dog_breed_labels_path = 'Dog Breed/labels.csv'\n",
    "\n",
    "# read addresses and labels\n",
    "labels = pd.read_csv(dog_breed_labels_path)\n",
    "\n",
    "# to shuffle data\n",
    "if shuffle_data:\n",
    "    labels = labels.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ojo: el siguiente bloque de código sólo se ejecuta una vez; sirve para mover los archivos de imágenes de cada clase a sus respectivas carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1000/8177\n",
      "Processing: original_index=8540, id=d66ec4c83a620cca6ebf05ab9d162fcd, label=cairn\n",
      "Train data: 2000/8177\n",
      "Processing: original_index=9036, id=e316925eb1cf7cdeb1ffaab7424e231d, label=shetland_sheepdog\n",
      "Train data: 3000/8177\n",
      "Processing: original_index=3264, id=511bfe35ff282294f6129c55bd6c33f6, label=malinois\n",
      "Train data: 4000/8177\n",
      "Processing: original_index=3337, id=52e7f48e18a9a55da9846d56821d5f69, label=cardigan\n",
      "Train data: 5000/8177\n",
      "Processing: original_index=8096, id=ca39d409bccf8034d2d11b7b44d8e0a1, label=vizsla\n",
      "Train data: 6000/8177\n",
      "Processing: original_index=4688, id=7589719a4f8dddeb1562fd1bff7f714e, label=schipperke\n",
      "Train data: 7000/8177\n",
      "Processing: original_index=8922, id=dfe45c3b288b8224eb17dbdaf1706496, label=irish_terrier\n",
      "Train data: 8000/8177\n",
      "Processing: original_index=9967, id=fa3bc3e096a2967f26113992b29b23b5, label=toy_terrier\n",
      "Train data: 8177/8177\n",
      "Processing: original_index=2584, id=403dc79fe2e7b3128ab675ef6762754a, label=labrador_retriever\n",
      "Validation data: 1000/2045\n",
      "Processing: original_index=4888, id=7aa633ff24f42d47caa54717d50bd2f7, label=miniature_pinscher\n",
      "Validation data: 2000/2045\n",
      "Processing: original_index=5182, id=82f33433170f776e0625acf56e1b63d3, label=groenendael\n",
      "Validation data: 2045/2045\n",
      "Processing: original_index=2842, id=45fef015b1974e98da0173e8260b3482, label=tibetan_mastiff\n"
     ]
    }
   ],
   "source": [
    "# load dataset and labels into variables\n",
    "\n",
    "# use 20% of the train set to create a validation set and another 20% for a test set\n",
    "train_labels = labels.iloc[:int(0.8*len(labels))]\n",
    "val_labels = labels.iloc[int(0.8*len(labels)):]\n",
    "\n",
    "# a numpy array to save the mean of the images\n",
    "\n",
    "# loop over train addresses\n",
    "for i, (index, img_id, label) in enumerate(train_labels.itertuples()):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if (i+1) % 1000 == 0 or i+1 == len(train_labels):\n",
    "        print ('Train data: {0}/{1}'.format(i+1, len(train_labels)))\n",
    "        print('Processing: original_index={0}, id={1}, label={2}'.format(index, img_id, label))\n",
    "\n",
    "    # move the image to a subdirectory named after its label\n",
    "    if not os.path.isdir(os.path.join(dog_breed_train_path, label)):\n",
    "        os.mkdir(os.path.join(dog_breed_train_path, label))\n",
    "    os.rename(dog_breed_train_path+img_id+'.jpg', dog_breed_train_path+label+'/'+img_id+'.jpg')\n",
    "\n",
    "for i, (index, img_id, label) in enumerate(val_labels.itertuples()):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if (i+1) % 1000 == 0 or i+1 == len(val_labels):\n",
    "        print ('Validation data: {0}/{1}'.format(i+1, len(val_labels)))\n",
    "        print('Processing: original_index={0}, id={1}, label={2}'.format(index, img_id, label))\n",
    "\n",
    "    # move the image to a subdirectory named after its label\n",
    "    if not os.path.isdir(os.path.join(dog_breed_val_path, label)):\n",
    "        os.mkdir(os.path.join(dog_breed_val_path, label))\n",
    "    os.rename(dog_breed_train_path+img_id+'.jpg', dog_breed_val_path+label+'/'+img_id+'.jpg')\n",
    "\n",
    "# aqui va el centrado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hay que jugar un poco con la arquitectura de la CNN. Mi PC no aguanta un modelo que tenga más de ~12 millones de parámetros, así que las opciones son achicar las imágenes de entrada (pic_dimension) y/o disminuir la cantidad de neuronas del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_214 (Conv2D)          (None, 112, 112, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_121 (MaxPoolin (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_215 (Conv2D)          (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_122 (MaxPoolin (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_216 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_123 (MaxPoolin (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 456)               11440584  \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 456)               0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 456)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 120)               54840     \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 120)               0         \n",
      "=================================================================\n",
      "Total params: 11,588,672\n",
      "Trainable params: 11,588,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "n_classes = 120\n",
    "pic_dimension = 112\n",
    "\n",
    "modelo_1 = Sequential()\n",
    "modelo_1.add(Conv2D(32, (3, 3), padding='same', input_shape=(pic_dimension, pic_dimension, 3)))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Conv2D(64, (3, 3), padding='same'))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Conv2D(128, (3, 3), padding='same'))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#modelo_1.add(Dropout(0.3))\n",
    "modelo_1.add(Flatten())\n",
    "modelo_1.add(Dense(456))\n",
    "modelo_1.add(Activation('relu'))\n",
    "modelo_1.add(Dropout(0.5))\n",
    "modelo_1.add(Dense(n_classes))\n",
    "modelo_1.add(Activation('softmax'))\n",
    "modelo_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "modelo_1.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de datos del dataset es muy poco (10222 imagenes) respecto a la dimensionalidad del mismo. Por lo tanto, es útil aumentar artificialmente el dataset aplicando distorsiones aleatorias a cada imagen, de manera que el modelo en entrenamiento nunca vea la misma imagen más de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8177 images belonging to 120 classes.\n",
      "Found 2045 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for validation:\n",
    "# only rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        dog_breed_train_path,  # this is the target directory\n",
    "        target_size=(pic_dimension, pic_dimension),  # all images will be resized to 112x112\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        dog_breed_val_path,\n",
    "        target_size=(pic_dimension, pic_dimension),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "159/159 [==============================] - 65s 410ms/step - loss: 4.8881 - acc: 0.0101 - val_loss: 4.7579 - val_acc: 0.0166\n",
      "Epoch 2/50\n",
      "159/159 [==============================] - 56s 354ms/step - loss: 4.6953 - acc: 0.0224 - val_loss: 4.5743 - val_acc: 0.0277\n",
      "Epoch 3/50\n",
      "159/159 [==============================] - 57s 360ms/step - loss: 4.5506 - acc: 0.0296 - val_loss: 4.4368 - val_acc: 0.0393\n",
      "Epoch 4/50\n",
      "159/159 [==============================] - 57s 358ms/step - loss: 4.4329 - acc: 0.0408 - val_loss: 4.3152 - val_acc: 0.0585\n",
      "Epoch 5/50\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 4.3497 - acc: 0.0495 - val_loss: 4.2232 - val_acc: 0.0691\n",
      "Epoch 6/50\n",
      "159/159 [==============================] - 56s 351ms/step - loss: 4.2607 - acc: 0.0645 - val_loss: 4.1381 - val_acc: 0.0746\n",
      "Epoch 7/50\n",
      "159/159 [==============================] - 56s 353ms/step - loss: 4.1797 - acc: 0.0746 - val_loss: 4.1579 - val_acc: 0.0771\n",
      "Epoch 8/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 4.1447 - acc: 0.0744 - val_loss: 4.0736 - val_acc: 0.0887\n",
      "Epoch 9/50\n",
      "159/159 [==============================] - 58s 365ms/step - loss: 4.0525 - acc: 0.0859 - val_loss: 4.1115 - val_acc: 0.0867\n",
      "Epoch 10/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 4.0321 - acc: 0.0899 - val_loss: 4.0570 - val_acc: 0.0917\n",
      "Epoch 11/50\n",
      "159/159 [==============================] - 56s 354ms/step - loss: 3.9797 - acc: 0.0990 - val_loss: 4.0198 - val_acc: 0.0958\n",
      "Epoch 12/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.9197 - acc: 0.1079 - val_loss: 3.9343 - val_acc: 0.1190\n",
      "Epoch 13/50\n",
      "159/159 [==============================] - 57s 355ms/step - loss: 3.9068 - acc: 0.1143 - val_loss: 3.9013 - val_acc: 0.1154\n",
      "Epoch 14/50\n",
      "159/159 [==============================] - 58s 364ms/step - loss: 3.8631 - acc: 0.1150 - val_loss: 3.8918 - val_acc: 0.1144\n",
      "Epoch 15/50\n",
      "159/159 [==============================] - 58s 364ms/step - loss: 3.8183 - acc: 0.1205 - val_loss: 3.9043 - val_acc: 0.1184\n",
      "Epoch 16/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.7802 - acc: 0.1326 - val_loss: 3.9106 - val_acc: 0.1124\n",
      "Epoch 17/50\n",
      "159/159 [==============================] - 56s 355ms/step - loss: 3.7481 - acc: 0.1381 - val_loss: 3.8712 - val_acc: 0.1210\n",
      "Epoch 18/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.7237 - acc: 0.1346 - val_loss: 3.8469 - val_acc: 0.1316\n",
      "Epoch 19/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.6959 - acc: 0.1391 - val_loss: 3.8317 - val_acc: 0.1260\n",
      "Epoch 20/50\n",
      "159/159 [==============================] - 56s 355ms/step - loss: 3.6615 - acc: 0.1523 - val_loss: 3.7654 - val_acc: 0.1431\n",
      "Epoch 21/50\n",
      "159/159 [==============================] - 56s 355ms/step - loss: 3.6380 - acc: 0.1533 - val_loss: 3.8700 - val_acc: 0.1290\n",
      "Epoch 22/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.6280 - acc: 0.1526 - val_loss: 3.8671 - val_acc: 0.1346\n",
      "Epoch 23/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.6104 - acc: 0.1584 - val_loss: 3.8618 - val_acc: 0.1391\n",
      "Epoch 24/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.5837 - acc: 0.1635 - val_loss: 3.7696 - val_acc: 0.1457\n",
      "Epoch 25/50\n",
      "159/159 [==============================] - 56s 353ms/step - loss: 3.5846 - acc: 0.1617 - val_loss: 3.7542 - val_acc: 0.1406\n",
      "Epoch 26/50\n",
      "159/159 [==============================] - 56s 353ms/step - loss: 3.5414 - acc: 0.1653 - val_loss: 3.8220 - val_acc: 0.1477\n",
      "Epoch 27/50\n",
      "159/159 [==============================] - 56s 354ms/step - loss: 3.5535 - acc: 0.1655 - val_loss: 3.8438 - val_acc: 0.1436\n",
      "Epoch 28/50\n",
      "159/159 [==============================] - 55s 349ms/step - loss: 3.5408 - acc: 0.1710 - val_loss: 3.7452 - val_acc: 0.1562\n",
      "Epoch 29/50\n",
      "159/159 [==============================] - 56s 354ms/step - loss: 3.5238 - acc: 0.1740 - val_loss: 4.0498 - val_acc: 0.1230\n",
      "Epoch 30/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.5040 - acc: 0.1742 - val_loss: 3.7436 - val_acc: 0.1562\n",
      "Epoch 31/50\n",
      "159/159 [==============================] - 56s 353ms/step - loss: 3.5099 - acc: 0.1709 - val_loss: 3.7642 - val_acc: 0.1527\n",
      "Epoch 32/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.5080 - acc: 0.1787 - val_loss: 3.8579 - val_acc: 0.1366\n",
      "Epoch 33/50\n",
      "159/159 [==============================] - 56s 351ms/step - loss: 3.5092 - acc: 0.1753 - val_loss: 3.8385 - val_acc: 0.1316\n",
      "Epoch 34/50\n",
      "159/159 [==============================] - 59s 373ms/step - loss: 3.4790 - acc: 0.1791 - val_loss: 3.7887 - val_acc: 0.1643\n",
      "Epoch 35/50\n",
      "159/159 [==============================] - 56s 351ms/step - loss: 3.4589 - acc: 0.1895 - val_loss: 3.8379 - val_acc: 0.1573\n",
      "Epoch 36/50\n",
      "159/159 [==============================] - 56s 351ms/step - loss: 3.4720 - acc: 0.1794 - val_loss: 3.7759 - val_acc: 0.1421\n",
      "Epoch 37/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.4818 - acc: 0.1838 - val_loss: 3.8004 - val_acc: 0.1467\n",
      "Epoch 38/50\n",
      "159/159 [==============================] - 59s 368ms/step - loss: 3.4502 - acc: 0.1858 - val_loss: 3.8031 - val_acc: 0.1527\n",
      "Epoch 39/50\n",
      "159/159 [==============================] - 56s 349ms/step - loss: 3.4594 - acc: 0.1840 - val_loss: 4.0930 - val_acc: 0.1487\n",
      "Epoch 40/50\n",
      "159/159 [==============================] - 56s 349ms/step - loss: 3.4425 - acc: 0.1850 - val_loss: 3.7360 - val_acc: 0.1628\n",
      "Epoch 41/50\n",
      "159/159 [==============================] - 56s 351ms/step - loss: 3.4396 - acc: 0.1913 - val_loss: 3.7875 - val_acc: 0.1583\n",
      "Epoch 42/50\n",
      "159/159 [==============================] - 57s 361ms/step - loss: 3.4302 - acc: 0.1938 - val_loss: 3.7952 - val_acc: 0.1512\n",
      "Epoch 43/50\n",
      "159/159 [==============================] - 57s 358ms/step - loss: 3.4339 - acc: 0.1893 - val_loss: 3.7937 - val_acc: 0.1568\n",
      "Epoch 44/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.4623 - acc: 0.1888 - val_loss: 3.7755 - val_acc: 0.1391\n",
      "Epoch 45/50\n",
      "159/159 [==============================] - 56s 349ms/step - loss: 3.4281 - acc: 0.1973 - val_loss: 4.3012 - val_acc: 0.1628\n",
      "Epoch 46/50\n",
      "159/159 [==============================] - 57s 356ms/step - loss: 3.4562 - acc: 0.1911 - val_loss: 4.0066 - val_acc: 0.1643\n",
      "Epoch 47/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.4447 - acc: 0.1915 - val_loss: 3.8986 - val_acc: 0.1316\n",
      "Epoch 48/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.4257 - acc: 0.1962 - val_loss: 3.7333 - val_acc: 0.1663\n",
      "Epoch 49/50\n",
      "159/159 [==============================] - 56s 350ms/step - loss: 3.4352 - acc: 0.1947 - val_loss: 3.7390 - val_acc: 0.1608\n",
      "Epoch 50/50\n",
      "159/159 [==============================] - 56s 352ms/step - loss: 3.4517 - acc: 0.1926 - val_loss: 4.0441 - val_acc: 0.1709\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento de la CNN.\n",
    "import tensorflow as tf\n",
    "\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=conf)\n",
    "\n",
    "from keras import backend as k\n",
    "k.set_session(sess)\n",
    "\n",
    "results_test = {'test_loss': [], 'test_acc': []}\n",
    "\n",
    "results = modelo_1.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=10222//batch_size, # tamaño del dataset completo//tamaño del batch \n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=2045//batch_size,\n",
    "        #callbacks=[lrate]\n",
    "        )\n",
    "modelo_1.save_weights('modelo_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
